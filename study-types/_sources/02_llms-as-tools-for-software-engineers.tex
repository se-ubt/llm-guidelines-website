\documentclass[11pt]{article}
\usepackage[parfill]{parskip} % use newlines for paragraphs (more similar to Markdown)

\begin{document}

\subsection{LLMs as Tools for Software Engineers}

\textbf{TODO:} Write section intro 

\subsubsection{Studying LLM Usage in Software Engineering}

\emph{Description:} Empirical studies can also focus on understanding how software engineers use LLMs in their workflows. 

\emph{Examples:} This involves investigating the adoption, usage patterns, and perceived benefits and challenges of LLM-based tools. Surveys, interviews, and observational studies can provide insights into how LLMs are integrated into development processes, how they influence decision-making, and what factors affect their acceptance and effectiveness. Such studies can inform the design of more user-friendly and effective LLM-based tools.

\emph{Promises:} \textbf{TODO}

\emph{Perils:} \textbf{TODO}

\emph{Previous Work in SE:} Khojah et al. investigated the use of ChatGPT by professional software engineers in a week-long observational study~\cite{DBLP:journals/pacmse/KhojahM0N24}.

\subsubsection{LLMs for new Software Engineering Tools}

\emph{Description:} LLMs are being integrated into new tools designed to support software engineers in their daily tasks.

\emph{Examples:} These tools can include intelligent code editors that provide real-time code suggestions, automated documentation generators, and advanced debugging assistants. Empirical studies can evaluate the effectiveness of these tools in improving productivity, code quality, and developer satisfaction.

\emph{Promises:} By assessing the impact of LLM-powered tools, researchers can identify best practices and areas for further improvement.

\emph{Perils:} \textbf{TODO}

\emph{Previous Work in SE:}  Choudhuri et al. conducted an experiment with students in which they measured the impact of ChatGPT on the correctness and time taken to solve programming tasks~\cite{DBLP:conf/icse/ChoudhuriLSGS24}.


\subsubsection{Benchmarking LLMs for Software Engineering Tasks}

\emph{Description:} Benchmarking is the process of evaluating the LLM output obtained from a \textit{standardized} datasets using a set of \texit{standardized} metrics.
High-quality reference datasets, such as HumanEval~\cite{DBLP:journals/corr/abs-2107-03374} for the task of code generation, are necessary to perform evaluation across studies.
LLM output is compared against a ground truth from the dataset in the benchmark using general metrics for text generation, such as ROUGE, BLEU, or METEOR~\cite{10.1145/3695988}, a well as task-specific metrics, such as Pass@k for code generation.

\emph{Examples:} In software engineering, benchmarking may include the evaluation of LLMs' ability to produce accurate and robust outputs for input data from curated real-world projects or synthetic SE specific datasets.

RepairBench~\cite{silva2024repairbench}, for example, contains 574 buggy Java methods and their corresponding fixed versions, which can be used to evaluate the performance of LLMs in code repair tasks.
The metrics are Plausible@1 (i.e., the probability that the first generated patch passes all test cases) and AST Match@1 (i.e., the probability that the Abstract Syntax Tree of the first generated patch matches the one of the ground truth patch).

SWE-Bench~\cite{DBLP:conf/iclr/JimenezYWYPPN24} is a more generic benchmark that contains 2,294 SE python tasks extracted from GitHub pull requests.
For scoring the performance of the LLMs on the tasks, the authors report first report whether the generated patch is applicable or not (i.e., it fails compilation) and, for successful patches, they use the percentage of test cases passed.

\emph{Promises:} \textbf{TODO}

\emph{Perils:} Benchmark contamination~\cite{DBLP:journals/corr/abs-2410-16186} has recently been identified as an issue.
The careful selection of samples and building of corresponding input prompts is particularly important, as correlations between prompts may bias benchmark results~\cite{DBLP:conf/acl/SiskaMAB24}.

\emph{Previous Work in SE:} \textbf{TODO}

\subsubsection{References}

\bibliographystyle{plain}
\bibliography{../../literature.bib}

\end{document}

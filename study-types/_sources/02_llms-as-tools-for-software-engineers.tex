\documentclass[11pt]{article}
\usepackage[parfill]{parskip} % use newlines for paragraphs (more similar to Markdown)
\newcommand{\todo}[1]{{\textbf{TODO:}\ \textit{#1}}} % command for TODOs
\usepackage{hyperref}

\begin{document}

\subsection{LLMs as Tools for Software Engineers}

\todo{Write section intro}

\subsubsection{Studying LLM Usage in Software Engineering}

\emph{Description:} Empirical studies can also focus on understanding how software engineers use LLMs in their workflows. 

\emph{Example(s):} This involves investigating the adoption, usage patterns, and perceived benefits and challenges of LLM-based tools. Surveys, interviews, and observational studies can provide insights into how LLMs are integrated into development processes, how they influence decision-making, and what factors affect their acceptance and effectiveness. Such studies can inform the design of more user-friendly and effective LLM-based tools.
For example, Khojah et al. investigated the use of ChatGPT by professional software engineers in a week-long observational study~\cite{DBLP:journals/pacmse/KhojahM0N24}.

\emph{Advantages:} \todo{write paragraph}

\emph{Challenges:} \todo{write paragraph}


\subsubsection{LLMs for new Software Engineering Tools}

\emph{Description:} LLMs are being integrated into new tools designed to support software engineers in their daily tasks.

\emph{Example(s):} These tools can include intelligent code editors that provide real-time code suggestions, automated documentation generators, and advanced debugging assistants. Empirical studies can evaluate the effectiveness of these tools in improving productivity, code quality, and developer satisfaction. For example, Choudhuri et al. conducted an experiment with students in which they measured the impact of ChatGPT on the correctness and time taken to solve programming tasks~\cite{DBLP:conf/icse/ChoudhuriLSGS24}.

\emph{Advantages:} By assessing the impact of LLM-powered tools, researchers can identify best practices and areas for further improvement.

\emph{Challenges:} \todo{write paragraph}


\subsubsection{Benchmarking LLMs for Software Engineering Tasks}

\emph{Description:} Benchmarking is the process of evaluating the LLM output obtained from a \emph{standardized} datasets using a set of \emph{standardized} metrics.
High-quality reference datasets, such as HumanEval~\cite{DBLP:journals/corr/abs-2107-03374} for the task of code generation, are necessary to perform evaluation across studies.
LLM output is compared against a ground truth from the dataset in the benchmark using general metrics for text generation, such as ROUGE, BLEU, or METEOR~\cite{10.1145/3695988}, a well as task-specific metrics, such as Pass@k for code generation.

\emph{Example(s):} In software engineering, benchmarking may include the evaluation of LLMs' ability to produce accurate and robust outputs for input data from curated real-world projects or synthetic SE specific datasets. Typical tasks include code generation, code summarization, code completion, and code repair, but also natural-language processing tasks---i.e., anaphora resolution---interesting for subfields such a Requirements Engineering. 
RepairBench~\cite{silva2024repairbench}, for example, contains 574 buggy Java methods and their corresponding fixed versions, which can be used to evaluate the performance of LLMs in code repair tasks.
The metrics are Plausible@1 (i.e., the probability that the first generated patch passes all test cases) and AST Match@1 (i.e., the probability that the Abstract Syntax Tree of the first generated patch matches the one of the ground truth patch).
SWE-Bench~\cite{DBLP:conf/iclr/JimenezYWYPPN24} is a more generic benchmark that contains 2,294 SE python tasks extracted from GitHub pull requests.
For scoring the performance of the LLMs on the tasks, the authors report first report whether the generated patch is applicable or not (i.e., it fails compilation) and, for successful patches, they use the percentage of test cases passed.

\emph{Advantages:} Properly-built benchmarks provide objective evaluation across different tasks, enabling fair comparison of different models (and versions).
Moreover, benchmarks build for specific SE tasks can help identify weaknesses of LLM and support their optimization/fine-tuning for such tasks.
Benchmark built using real-world data can also help legitimize research results for practitioners, supporting industry-academia collaboration.
Finally, benchmarks can foster open science practices, by providing a common ground for sharing data (e.g., as part of the benchmark itself) and results (e.g., of models run against a benchmark).

\emph{Challenges:} Benchmark contamination~\cite{DBLP:journals/corr/abs-2410-16186} has recently been identified as an issue.
The careful selection of samples and building of corresponding input prompts is particularly important, as correlations between prompts may bias benchmark results~\cite{DBLP:conf/acl/SiskaMAB24}.
Recently, Cao et al.~\cite{cao2025should} has proposed guidelines for building benchmarks for LLMs related to coding tasks, grounded in a systematic survey of existing benchmarks. 
In this process, they highlight current shortcomings related to reliability, transparency irreproducibility, low data quality, and inadequate validation measures.


\subsubsection{References}

\bibliographystyle{plain}
\bibliography{../../literature.bib}

\end{document}

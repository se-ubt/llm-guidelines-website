\documentclass[11pt]{article}
\usepackage[parfill]{parskip} % use newlines for paragraphs (more similar to Markdown)

\begin{document}

\subsection{LLMs as Tools for Software Engineers}

\textbf{TODO:} Write section intro 

\subsubsection{Studying LLM Usage in Software Engineering}

\emph{Description:} Empirical studies can also focus on understanding how software engineers use LLMs in their workflows. 

\emph{Examples:} This involves investigating the adoption, usage patterns, and perceived benefits and challenges of LLM-based tools. Surveys, interviews, and observational studies can provide insights into how LLMs are integrated into development processes, how they influence decision-making, and what factors affect their acceptance and effectiveness. Such studies can inform the design of more user-friendly and effective LLM-based tools.

\emph{Promises:} \textbf{TODO}

\emph{Perils:} \textbf{TODO}

\emph{Previous Work in SE:} Khojah et al. investigated the use of ChatGPT by professional software engineers in a week-long observational study~\cite{DBLP:journals/pacmse/KhojahM0N24}.

\subsubsection{LLMs for new Software Engineering Tools}

\emph{Description:} LLMs are being integrated into new tools designed to support software engineers in their daily tasks.

\emph{Examples:} These tools can include intelligent code editors that provide real-time code suggestions, automated documentation generators, and advanced debugging assistants. Empirical studies can evaluate the effectiveness of these tools in improving productivity, code quality, and developer satisfaction.

\emph{Promises:} By assessing the impact of LLM-powered tools, researchers can identify best practices and areas for further improvement.

\emph{Perils:} \textbf{TODO}

\emph{Previous Work in SE:}  Choudhuri et al. conducted an experiment with students in which they measured the impact of ChatGPT on the correctness and time taken to solve programming tasks~\cite{DBLP:conf/icse/ChoudhuriLSGS24}.


\subsubsection{Benchmarking LLMs for Software Engineering Tasks}

\emph{Description:}  Another typical type of study focuses on benchmarking the LLM output quality on large-scale datasets. In these benchmarks, reference datasets such as HumanEval~\cite{DBLP:journals/corr/abs-2107-03374} play an important role in establishing standardized evaluation methods across studies. LLM output is often compared against a ground truth from the dataset using similarity metrics such as ROUGE, BLEU, or METEOR~\cite{10.1145/3695988}.
Moreover, the evaluation may be augmented by task-specific measures that focus on the type of SE artifact produced.

\emph{Examples:} In software engineering, benchmarking may include the evaluation of LLMs' ability to produce accurate and robust outputs for input data from real-world projects or synthetically created SE datasets.
Used metrics can include code quality or performance metrics for code generation tasks or metrics for writing quality in SE tasks with natural language artifacts, such as requirements documents or domain descriptions.

\emph{Promises:} \textbf{TODO}

\emph{Perils:} Benchmark contamination~\cite{DBLP:journals/corr/abs-2410-16186} has recently been identified as an issue.
The careful selection of samples and building of corresponding input prompts is particularly important, as correlations between prompts may bias benchmark results~\cite{DBLP:conf/acl/SiskaMAB24}.

\emph{Previous Work in SE:} \textbf{TODO}

\subsubsection{References}

\bibliographystyle{plain}
\bibliography{../../literature.bib}

\end{document}

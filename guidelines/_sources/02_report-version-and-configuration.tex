\input{../../header.tex}

\begin{document}

\subsection{Report Model Version and Configuration}

\subsubsection{Recommendations}

\todo{indicate what information is supposed to be reported in the \paper or in the \supplementarymaterial}

\todo{double-check usage of \must, \should, etc.}

\todo{Add a recommendations that researchers \must motivate why certain models, versions, and configurations were selected. For example due to monetary or technical reasons, based on previous work using the same models, etc. This applies also the the model size, as smaller models might have been selected due to hardware constraints. This is totally fine, but needs to be reported.}

LLMs or LLM-based tools, especially those offered as-a-service, are frequently updated; different versions may produce varying results for the same input.
Moreover, configuration parameters such as the temperature affect content generation.
Therefore, researchers \must document the specific model or tool version used in a study, along with the date when the experiments were conducted, and the exact configuration being used.
Since default values might change over time, researchers \should always report all configuration values, even if they used the defaults.
Depending on the specific study context, additional information regarding the architecture of the tool or experiment \should be reported (see Section \href{/guidelines/#report-tool-architecture-and-supplemental-data}{Report Tool Architecture and Supplemental Data}).
Our recommendation is to report:

\begin{itemize}
\item Model/tool name.
\item Model/tool version (including a checksum if available).
\item The configured temperature that controls randomness, and all other relevant parameters that affect output generation (e.g., seed values).
\item The context window (number of tokens).
\item Whether historical context was considered when generating responses.
\end{itemize}

\comment{I think we should add a section related to fine tuning? It is becoming more and more common and without that information reproducibility is heavily affected. For example:
``For fine-tuned models, researchers \textbf{must} additionally report:
\begin{itemize}
\item Base model used before fine-tuning
\item Fine-tuning dataset characteristics (size, source, preprocessing steps)
\item Fine-tuning hyperparameters (learning rate, epochs, batch size)
\item Training objective (e.g., next token prediction, instruction tuning)
\item Validation metrics used during fine-tuning
\item If possible, a link to the fine-tuned model weights or a description of how to access them.''
\end{itemize}
}

\comment{This also leads me to another point. In my review of LLMs as annotators, I've found several examples of ensemble models. I think we might also want to add a section about ensemble models. See example below.}

When multiple models are used together (e.g., in ensemble or pipeline architectures), researchers \textbf{MUST} additionally:
\begin{itemize}
\item Report version and configuration details for each model separately;
\item Describe the interaction between models;
\item Explain the architecture for combining outputs (e.g., majority voting, weighted averaging, sequential processing);
\item Document any logic that determines which model handles which inputs.
\end{itemize}

\subsubsection{Example(s)}

For an OpenAI model, researchers might report that ``A  \texttt{gpt-4} model was integrated via the Azure OpenAI Service, and configured with a temperature of 0.7, top\_p set to 0.8, and a maximum token length of 512. We used version \texttt{0125-Preview}, system fingerprint \texttt{fp\_6b68a8204b}, seed value \texttt{23487}, and ran our experiment on 10th January 2025''~\cite{OpenAI25, Azure25}.
Similar statements can be made for self-hosted models, for which supplementary material can report specific instructions for reproducing results.
For example, for models provisioned using \href{https://ollama.com/library/}{ollama}, one can report the specific tag and checksum of the model being used, e.g., `llama3.3, tag 70b-instruct-q8\_0, checksum d5b5e1b84868`.
Given suitable hardware, running the corresponding model in its default configuration is then as easy as executing \texttt{ollama run llama3.3:70b-instruct-q8\_0} (see Section \href{/guidelines/#use-an-open-llm-as-a-baseline}{Use an Open LLM as a Baseline}).

Kang et al.~provide a similar statement in their paper on exploring LLM-based general bug reproduction~\cite{DBLP:conf/icse/KangYY23}:

\begin{quote}
\it
``We access OpenAI Codex via its closed beta API, using the code-davinci-002 model. For Codex, we set the temperature to 0.7, and the maximum number of tokens to 256.''
\end{quote}

Our guidelines additionally suggest to report a checksum and exact dates, but otherwise this example is close to our recommendations. 

\comment{Commercial tools have limited information. This is a bit tricky. What about a little more guidance? See example below.}

When studying commercial tools where configuration access is limited:
\begin{itemize}
\item Report product name, version number, and access date (e.g., "GitHub Copilot in Visual Studio Code v1.2.3, accessed between January-March 2024");
\item Document any user-configurable settings and their values, such as temperature and maximum tokens (e.g., "For Codex, we set the temperature to 0.7, and the maximum number of tokens to 256.");
\item Archive interaction logs showing tool behavior (see section \href{/guidelines/report-interaction-logs}{Report Interaction Logs});
\item Acknowledge limitations in reproducibility due to the closed nature of the tool.
\end{itemize}


\subsubsection{Advantages}

The recommended information is a prerequisite to enable reproducibility of LLM-based studies under the same or similar conditions.
Please note that this information alone is generally not sufficient.
Therefore, depending on the specific study setup, researchers \should provide additional information about architecture and data (\href{/guidelines/#report-tool-architecture-and-supplemental-data}{Report Tool Architecture and Supplemental Data}), prompts (\href{/guidelines/#report-prompts-and-their-development}{Report Prompts and their Development}), interaction logs (\href{/guidelines//#report-interaction-logs}{Report Interaction Logs}), and specific limitations an mitigations (\href{/guidelines/#report-limitations-and-mitigations}{Report Limitations and Mitigations}).

\subsubsection{Challenges}

Different model providers and modes of operating the models allow for varying degrees of information.
For example, OpenAI provides a model version and a system fingerprint describing the backend configuration that can also influence the output.
However, the fingerprint is indeed just intended to detect changes to the model or its configuration.
As a user, one cannot go back to a certain fingerprint.
As a beta feature, OpenAI also lets users set a seed parameter to receive ``(mostly) consistent output''~\cite{OpenAI23}.
However, the seed value does not allow for full reproducibility and the fingerprint changes frequently. 
While, as motived above, open models significantly simplify re-running experiments, they also come with challenges in terms of reproducibility, as generated outputs can be inconsistent despite setting the temperature to 0 and using a seed value (see \href{https://github.com/ollama/ollama/issues/5321}{GitHub issue for Llama3}).

\comment{Should we also address how API rate limits and usage constraints can affect experiments?}

\subsubsection{Study Types}

This guideline \must be followed for all study types for which the researcher has access to (parts of) the model's configuration.
They \must always report the configuration that is visible to them, acknowledging the reproducibility challenges of commercial tools and models offered as-a-service. 
When \href{/study-types/#studying-llm-usage-in-software-engineering}{Studying LLM Usage in Software Engineering}, for example the usage of commercial tools such as ChatGPT or GitHub Copilot, researchers \must be as specific as possible in describing their study setup.
The model name and date \must always be reported.
In those cases, reporting other aspects such as prompts (\href{/guidelines/#report-prompts-and-their-development}{Report Prompts and their Development}) and interaction logs (\href{/guidelines//#report-interaction-logs}{Report Interaction Logs}) is essential.


\subsubsection{References}

\bibliographystyle{plainnat}
\bibliography{../../literature.bib}

\end{document}

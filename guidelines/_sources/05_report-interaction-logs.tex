\input{../../header.tex}

\begin{document}

\subsection{Report Interaction Logs}

\subsubsection{Recommendations}

% When reproducibility is important and transparency is needed, researchers \should report full interaction logs, that is, all prompts and responses generated by the LLM or LLM-based tool in the context of the presented study. Reporting this is especially important when reporting a study targeting commercial SaaS solutions based on LLMs (e.g., ChatGPT) or novel tools that integrate LLMs via cloud APIs where there is even less guarantee of reproducing the state of the LLM-powered system at a later point by a reader of the study who wants to replicate the part of the study that is downstream from the text-generation. 

Previous guidelines aim to address the reproducibility problem by encouraging the reporting of full version and parameters, but this will still not be always sufficient. Indeed, LLMs can still behave non-deterministically even if decoding strategies and parameters are fixed because non-determinism can arise from batching, input preprocessing, and floating point arithmetic on GPUs ~\cite{Chann2023}. Thus, in order to establish a fixed point from which a given study can be reproducible, a study \should report the full interaction logs with a LLM if possible. 

Reporting this is especially important when reporting a study targeting commercial SaaS solutions based on LLMs (e.g., ChatGPT) or novel tools that integrate LLMs via cloud APIs where there is even less guarantee of reproducing the state of the LLM-powered system at a later point by a reader of the study who wants to replicate it. 

To intuitively explain why this can be important, consider a study in which the researchers evaluated the correctness of bug fixing capabilities of LLM-based tools and consider that the researchers only provided the prompt without the LLM answer. One of the multiple prompts would include the buggy function below in which the function returns the wrong variable. 

```
You are a coding assistant. Below is a Python function that is buggy. Analyze the code and suggest a fix.
Code:

def remove_duplicates_buggy(input_list):
    unique_list = []
    for item in input_list:
        if item not in unique_list:
            unique_list.append(item)
    return input_list 
'''

If a paper doesn't include the output and simply states that the fix was correct, it lacks crucial information needed for reproducibility. Without this information, fellow researchers can't assess whether the solution was efficient or if it was written in a non-idiomatic way (e.g., the function could have been implemented more elegantly using Python list comprehensions). There could be other potential issues that researchers won't be able to verify if the detailed response is not provided.

This guideline is similar to reporting interview transcripts in qualitative research. In both cases, it's important to document the entire interaction between the interviewer and the participant. Just as a human participant might give different answers to the same question asked two months apart, the responses from OpenAI ChatGPT can also vary over time. Therefore, keeping a record of the actual conversation is crucial for accuracy and context.

% \comment{the intro does not make it super clear *why* this helps reproducibility. If the AI has changed, how does the log really help us, as the answer might be different. I would argue, like in qual research, the value is to show the researchers have engaged substnatially with the respondent, i.e. that the process was not superficial}
% \comment{I also don't really see a clear distinction with report prompts and their development. This one is arguing for transcripts of the interactions? }


\subsubsection{Example(s)}

In their paper ``Investigating ChatGPT's Potential to Assist in Requirements Elicitation Processes'' \cite{ronanki2023investigating}, Ronanki et al. report the full answers of ChatGPT and they upload them in a Zenodo record \href{https://zenodo.org/records/8124936}. 

\comment{I would suggest expanding on the details a bit, to support the point of transparency made earlier. Why is this record helpful?}


\subsubsection{Advantages}

The advantage of following this guideline is the transparency and increased reproducibility of the resulting research. 

Moreover, the guideline is easy to follow. Transcripts are easy to obtain (if we continue with the mental model of a LLM as an interviewee, this is especially evident in contrast with obtaining transcripts with human users). Even for systems where the interaction is based on voice, the interaction is first translated to text using speech-to-text methods, so it can also be easily obtained. In this sense, there is no excuse for researchers not reporting full transcripts. 

One other advantage is that, while for human participants conversations often cannot be reported due to confidentiality, LLM conversations can (e.g. as of beginning of 2025, the for-profit OpenAI company allows the sharing of chat transcripts: https://openai.com/policies/sharing-publication-policy/). 


Another advantage is that a future replication paper might be able to compare future results for the same prompts with past results for the LLM.


\subsubsection{Challenges}

Given that {\em chat transcripts} are easy to generate, a study might end up with a very large appendix. Consequently, online storage might be needed. Services such as \href{https://zenodo.org}{zenodo}, \href{https://figshare.com/}{figshare}, or other similar long term storage for research artifacts \should be used in such situations.

Not all systems allow the reporting of interaction logs with the same ease. E.g. chat bot systems are easy to report the interactions with, while auto-complete systems like GitHub Copilot, will be much harder to report. Indeed, the fact that Copilot provided a recommendation during a coding session can not be replicated unless one re-creates the whole codebase state at that given point in time -- which seems quite unrealistic. One way to report that would be sharing a screencast of the coding session. Another one is to use version control to report the state of the system when a recommendation was made.

% \comment{I think actually these tools do have logging, eg. https://docs.github.com/en/copilot/troubleshooting-github-copilot/viewing-logs-for-github-copilot-in-your-environment}
% \comment{Mircea: the logs are for debugging things that don't work }


\subsubsection{Study Types}

This guideline \should be followed for all study types. 
\comment{i agree with \should }

\subsubsection{References}
\comment{capitalization is not right in the bib entries}

\bibliographystyle{plain}
\bibliography{../../literature.bib}

\end{document}

\input{../../header.tex}

\begin{document}

\subsection{Report Interaction Logs}

\subsubsection{Recommendations}

Previous guidelines aim to address the reproducibility problem in this context, but even reporting full version and parameters might not be always sufficient. Indeed, LLMs can still behave non-deterministically even if parameters are fixed~\cite{Chann2023}: while decoding strategies and parameters can be fixed by defining seeds, setting temperature to 0, using deterministic decoding strategies etc., non-determinism can arise from batching, input preprocessing, and floating point arithmetic on GPUs. 
\comment{the intro is a bit hard to parse. what is 'this context'? I would suggest switching para 1 and para 2 to give a gentler intro to this section}

This is why, when reproducibility is important and transparency is needed, researchers should report full interaction logs, that is, all prompts and responses generated by the LLM or LLM-based tool in the context of the presented study. 
Reporting this is especially important when reporting a study targeting commercial SaaS solutions based on LLMs (e.g., ChatGPT) or novel tools that integrate LLMs via cloud APIs where there is even less guarantee of reproducing the state of the LLM-powered system at a later point by a reader of the study who wants to replicate it. 
\comment{add the SHOULD latex command in here where needed. }

In a sense, this is not different than reporting results of interviews with human studies in qualitative studies that include interviews: there researchers also report the full interaction between the interviewers and the participants. Intuitively, this is the same because both a human participant and the OpenAI ChatGPT might {\em change their answers} if asked the same question at two months distance; thus the transcript of the actual conversation is important to be tracked.
\comment{I would call "results of interviews" transcripts - i think this is what you mean. This is also called verifiability in qual research}
\comment{the intro does not make it super clear *why* this helps reproducibility. If the AI has changed, how does the log really help us, as the answer might be different. I would argue, like in qual research, the value is to show the researchers have engaged substnatially with the respondent, i.e. that the process was not superficial}
\comment{I also don't really see a clear distinction with report prompts and their development. This one is arguing for transcripts of the interactions? }


\subsubsection{Example(s)}

In their paper ``Investigating ChatGPT's Potential to Assist in Requirements Elicitation Processes'' \cite{ronanki2023investigating}, Ronanki et al. report the full answers of ChatGPT and they upload them in a Zenodo record \href{https://zenodo.org/records/8124936}. 

\todo{QUESTION: Does anybody have a better example? I could not find one}. 
\comment{not off the top of my head, but many blogs do report how they use AI}
\comment{I would suggest expanding on the details a bit, to support the point of transparency made earlier. Why is this record helpful?}


\subsubsection{Advantages}

The advantage of following this guideline is the transparency and reproducibility of the resulting research. 

Moreover, the guideline is easy to follow. Transcripts are easy to obtain (if we continue with the mental model of a LLM as an interviewee, this is especially evident in contrast with obtaining transcripts with human users). Even for systems where the interaction is based on voice, the interaction is first translated to text using speech-to-text methods, so it can also be easily obtained. In this sense, there is no excuse for researchers not reporting full transcripts. 

One other advantage is that, while for human participants conversations often cannot be reported due to confidentiality, LLM conversations can (e.g. as of beginning of 2025, the for-profile OpenAI company allows the sharing of chat transcripts: https://openai.com/policies/sharing-publication-policy/). 

\comment{i think another was to build on earlier studies, which maybe is implied in the reproducibility part, but I think could be explicit. I can now use Mircea's earlier interactions and avoid unfruitful paths}


\subsubsection{Challenges}

Given that {\em chat transcripts} are easy to generate, a study might end up with a very large appendix. Consequently, online storage might be needed. Services such as Zenodo, or other long term storage for research artifacts, will likely have to be used in such situations.
\comment{add \\href\{\} to create a link}

Not all systems allow the reporting of interaction logs with the same ease. E.g. chat bot systems are easy to report the interactions with; OTOH, auto-complete systems, like GitHub Copilot, will be much harder to report. Indeed, the fact that CoPilot provided a recommendation during a coding session can not be replicated unless one re-creates the whole codebase state at that given point in time (and that of GitHub Copilot too). One way to report that would be sharing a screencast of the coding session. But this might be found to be too troublesome by some. 
\comment{expand to "on the other hand". Standardize the spelling/caps of Copilot.}
\comment{I think actually these tools do have logging, eg. https://docs.github.com/en/copilot/troubleshooting-github-copilot/viewing-logs-for-github-copilot-in-your-environment}
\comment{I would delete the last sentence}

\subsubsection{Study Types}

This guideline \should be followed for all study types. 
\comment{i agree with \should }

\subsubsection{References}
\comment{capitalization is not right in the bib entries}

\bibliographystyle{plain}
\bibliography{../../literature.bib}

\end{document}

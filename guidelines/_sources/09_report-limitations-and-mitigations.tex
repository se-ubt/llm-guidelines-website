\input{../../header.tex}

\begin{document}

\subsection{Report Limitations and Mitigations}

\subsubsection{Recommendations}
When using large language models (LLMs) in empirical studies for software engineering, researchers face unique challenges and potential limitations that can influence the validity, reliability, and reproducibility~\cite{sallou2024breaking} of their findings including:

\textbf{Reproducibility}:
\begin{itemize}
  \item Non-deterministic outputs of LLMs: LLMs may produce non-deterministic results due to inherent randomness in their outputs, especially when a temperature setting greater than 0 is used.
  \item Sampling/Evaluation Bias: The selection of testing dataset, and researchers evaluating LLM outputs manually may introduce subjective bias or cognitive fatigue, particularly for large datasets.
  \item Model update/version:  The behavior of LLMs can depend on external factors such as API updates, model version changes and very sensitive to Prompt Variations, even because of time-based output drift.
  \item Closed source model leading to model evolution unpredictability. Researchers may lack insight into the model's inner workings, hindering the ability to fully understand and trust its outputs.
\end{itemize}

\textbf{Generalization}:
Even though the topic of generalizability is not new, it has gained new relevance with the increasing interest in LLMs. In LLM studies generalizability boils down to two main concerns: 
\begin{itemize}
  \item First, are the results specific to a LLM or can they be achieved with other LLMs too? If generalizability to other LLMs is not in scope of the research, this MUST be clearly explained. If in scope, researchers MUST compare their results or subsets of the results (if not possible e.g. due to computational cost) with other LLMs to assess the generalizability of their findings. If possible, researchers SHOULD also employ open LLMs to set a reproducible baseline (see Use Open LLM as a Baseline).
  \item Second, will these results still be valid in the future? Multiple studies (~\cite{DBLP:journals/corr/abs-2307-09009}, ~\cite{doi:10.1148/radiol.232411}) found, that the performance of ChatGPT within the same version drastically decreased over time. Reporting the model version and configuration is not sufficient in such cases. To date, the only way of migitating this limitation is the usage of an Open LLM with a transparently communicated versioning and archiving (see Use Open LLM as a Baseline). Hence researchers SHOULD employ such models. If this is not possible, researchers SHOULD test and report their results over an extended period of time as a proxy of the results' relevance over time.
\end{itemize}

\textbf{Data Leakage}:

\comment{This paper also seems related: \cite{zhou2025lessleakbenchinvestigationdataleakage}}

Data leakage/contamination/overfitting occurs when information from outside the training dataset influences the model, leading to overly optimistic performance estimates. With the growing reliance on big datasets, the risks of inter-dataset duplication (i.e. samples duplication between datasets) increases (e.g. ~\cite{10.1145/3133908}, ~\cite{10.1145/3359591.3359735}). 
In the context of LLMs for software engineering, this can manifest as inter-dataset duplication, where samples from one dataset (e.g. pre-train dataset) appear in another (e.g. fine-tune dataset), potentially compromising the validity of evaluation results ~\cite{inter-dataset-lopez2025}. 
Hence, to ensure the validity of the evaluation results, researchers SHOULD carefully select the fine-tuning and evaluation datasets to prevent inter-dataset duplication. If information about the pre-train dataset of the employed LLM is available, researchers SHOULD assess the inter-dataset duplication and MUST discuss the potential data leakage.
In addition, researchers MUST NOT leak their fine-tune or evaluation datasets into the improvement process of the LLM (e.g. OpenAI's functionality called "Improve the model for everyone"), as this potentially impacts further evaluation rounds (e.g. think about pass@k), and exacerbates the issue of reproducibility.
If training a LLM from scratch, researchers MAY consider to use open datasets (such as Together AI's RedPajama~\cite{together2023redpajama}) that already incorporate duplication (with the positive side-effect of improving performance ~\cite{lee-etal-2022-deduplicating}).

\textbf{Scalability and Cost}:
Conducting studies with LLMs is a resource demanding endeavour. For self-hosted LLMs, the respective hardware needs to be provided, for managed LLMs, the service cost has to be considered. Thie challenge becomes more pronounced as LLMs grow larger, research architectures get more complex, and experiements become more computationally expensive e.g. multiple repetitions to assess performance in the face of non-determinism (see Report Suitable Baselines, Benchmarks, and Metrics).
Consequently, resource-intensive research remains predominantly the domain of well-funded researchers, hindering researcher with limited resources from replicating or extending the study results.
Hence, for transparency reasons, researchers SHOULD report cost associated with executing the study. If the study employed self-hosted LLMs, researchers SHOULD report the hardware used. If the study employed managed LLMs, the service cost SHOULD be reported.
To ensure research result validity and replicability, researchers MUST provide the LLM outputs as evidence for validation at different granularities (e.g. when employing multi agent systems). Additionally, researchers SHOULD include a sample, selected using an accepted sampling strategy, to allow partial replication of results.

\textbf{Misleading Performance Metrics}: TODO
Metrics like BLEU or ROUGE, often used in text-based tasks, may not capture important aspects of software engineering tasks (e.g., functional correctness, runtime performance).

\textbf{Ethical Concerns with Proprietary Data}: TODO
Using LLMs for tasks involving proprietary or sensitive software engineering data (e.g., private bug reports), especially in closed-source LLMs may raise privacy and IP concerns.

\textbf{Clear info:} TODO
\begin{itemize}
  \item Number of repetitions (``When evaluating model performance, it is recommended to conduct multiple tests and average the results.'' \url{https://huggingface.co/deepseek-ai/DeepSeek-R1\#usage-recommendations}), how were repetitions aggregated?, discuss limitations and mitigations
  \item Model version, fixed random seeds
  \item etc...
\end{itemize}

\textbf{Performance and Resource Consumption}: TODO
\todo{Discuss aspects as such increased performance (see benchmarking guidelines) vs. increased resource consumption and non-determinism.}
\todo{Balance of evaluation dataset: ``We have chosen our test samples to be small enough to perform manual semantic analysis, but large enough to draw conclusions''~\cite{tinnessoftware}}

\subsubsection{Example(s)}
\textbf{Reproducibility}: TODO
~\cite{10.1145/3673791.3698432} "When comparing our results, with the results presented by Wang et al. [54], we saw similar trends to them, but were unable to repro- duce their results. In general, we found that our results are neither reproducible nor reliable when rerunning the same prompts with different seeds, which led to a high variance in the obtained re- sults. With the results fluctuating considerably, we do not consider any of the obtained results reliable enough for the critical task of systematic reviews."

\textbf{Data Leakage}:
Since much research in software engineering evolves around code, inter-dataset code duplication has been extensively researched and addressed over the years to curate deduplicated datasets (e.g. by Lopes in 2017 ~\cite{10.1145/3133908}, Allamanis in 2019 ~\cite{10.1145/3359591.3359735}, Karmakar in 2023 ~\cite{10.1007/s10664-022-10275-7}, or Lopez in 2025 ~\cite{inter-dataset-lopez2025}).
The issue of inter-dataset duplication has also attracted interest in other disciplines, with growing demands for data mining. For example in the biology field, Lakiotaki et al. ~\cite{10.1093/database/bay011} acknowledge and address the overlap between multiple commonly disease datasets. 
When employing Falcon LLMs the issue of inter-dataset duplication can be partially assessed and mitigated. The technology innovation institute publicly provides access to parts of its training data for the Falcon LLMs ~\cite{technology_innovation_institute_2023} via Huggingface.
To prevent actively leaking data yourself into a LLMs improvment process ensure, that the data is not used to train the model (e.g. via OpenAI's data control functionality, or the OpenAI API instead of the web interface) ~\cite{balloccu-etal-2024-leak}.

\textbf{Generalization}:
\begin{itemize}
  \item To analyze whether the results of proprietary LLMs transfer to open LLMs, Staudinger et al. ~\cite{10.1145/3673791.3698432} benchmarked previous results using GPT3.5 and GPT4 against Mistral and Zephyr. They found, that the employed open source models could not deliver the same performance as the proprietary models.
  \item Individual studies already started to highlight the uncertainty about the generalizability of their results in the future. In ~\cite{10174227} Jesse et al. acknowledge the issue that LLMs evolve over time and this evolution impact the generation.
\end{itemize}

\textbf{Scalability and Cost}:
For example, in their comparison of performance of proprietary and open LLMs Staudinger et al. specified the costs as "120 USD in API calls for GPT 3.5 and GPT 4, and 30 USD in API calls for Mistral AI. Thus, the total LLM-cost of our reproducibility study was 150 USD". ~\cite{10.1145/3673791.3698432}

\todo{write paragraph}

\subsubsection{Advantages}
\textbf{Data Leakage}:
Assessing and mitigating the effects of inter-dataset duplication strengthens a studies' validity and reliability, as it prevents overly optimistic performance estimates that do not apply to previously unkown samples.

\textbf{Generalization}:
Mitigating threats to generalizability through the integration of open LLMs as a baseline or the reporting of results over an extended period of time can increase the validity, reliability and replicability of a study's results.

\textbf{Scalability and Cost}:
Reporting the cost associated with executing a study not only increases transparency, but also supports secondary literature in setting primary research into perspective.
Providing replication packages entailing direct LLM output evidence as well as samples for partial replicability are paramount steps towards open and inclusive research in the light of resource inequality among researchers.

\todo{write paragraph}


\subsubsection{Challenges}
\textbf{Data Leakage}:
Most LLM providers do not publicly offer information about the datasets employed for pre-training, impeding the assessment of inter-dataset duplication effects.

\textbf{Generalization}:
With commercial LLMs evolving over time, the generalizability of results to future versions of the model is uncertain. Employing open LLMs as a baseline can mitigate this limitation, but may not always be feasible due to computational cost.

\textbf{Scalability and Cost}:
Consistently keeping track of the cost involved in a research endeavour is challenging.
Building a coherent replication package that includes LLM outputs and samples for partial replicability requires additional effort and resources.
\todo{write paragraph}


\subsubsection{Study Types}
\todo{Connect guideline to study types and for each type have bullet point lists with information that MUST, SHOULD, or MAY be reported (usage of those terms according to RFC 2119~\cite{rfc2119}).}


\subsubsection{References}

\bibliographystyle{plain}
\bibliography{../../literature.bib}

\end{document}

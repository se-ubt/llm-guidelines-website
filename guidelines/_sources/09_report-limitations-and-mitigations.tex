\input{../../header.tex}

\begin{document}

\subsection{Report Limitations and Mitigations}

\subsubsection{Recommendations}
When using large language models (LLMs) in empirical studies for software engineering, researchers face unique challenges and potential limitations that can influence the validity, reliability, and reproducibility~\cite{sallou2024breaking} of their findings including:

\textbf{Reproducibility and Generalization}:
\begin{itemize}
  \item Non-deterministic outputs of LLMs: LLMs may produce non-deterministic results due to inherent randomness in their outputs, especially when a temperature setting greater than 0 is used.
  \item Sampling/Evaluation Bias: The selection of testing dataset, and researchers evaluating LLM outputs manually may introduce subjective bias or cognitive fatigue, particularly for large datasets.
  \item Model update/version:  The behavior of LLMs can depend on external factors such as API updates, model version changes and very sensitivive to Prompt Variations, even because of time-based output drift.
  \item Closed source model leading to model evolution unpredictability. Researchers may lack insight into the model's inner workings, hindering the ability to fully understand and trust its outputs.
\end{itemize}

\textbf{Data Leakage}:
Data leakage/contamination/overfitting occurs when information from outside the training dataset influences the model, leading to overly optimistic performance estimates. With the growing reliance on big datasets, the risks of inter-dataset duplication (i.e. samples duplication between datasets) increases (e.g. ~\cite{10.1145/3133908}, ~\cite{10.1145/3359591.3359735}). 
In the context of LLMs for software engineering, this can manifest as inter-dataset duplication, where samples from one dataset (e.g. pre-train dataset) appear in another (e.g. fine-tune dataset), potentially compromising the validity of evaluation results ~\cite{inter-dataset-lopez2025}. 
Hence, to ensure the validity of the evaluation results, researchers SHOULD carefully select the fine-tuning and evaluation datasets to prevent inter-dataset duplication. If information about the pre-train dataset of the employed LLM model is available, researchers SHOULD assess the inter-dataset duplication and MUST discuss the potential data leakage.
If training a LLM from scratch, researchers MAY consider to use open datasets (such as Together AI's RedPajama~\cite{together2023redpajama}) that already incorporate duplication (with the positive side-effect of improving performance ~\cite{lee-etal-2022-deduplicating}).

\textbf{Scalability and Cost}:
Conducting multiple repetitions or large-scale experiments can be computationally expensive, especially with proprietary APIs.

\textbf{Misleading Performance Metrics}:
Metrics like BLEU or ROUGE, often used in text-based tasks, may not capture important aspects of software engineering tasks (e.g., functional correctness, runtime performance).

\textbf{Ethical Concerns with Proprietary Data}:
Using LLMs for tasks involving proprietary or sensitive software engineering data (e.g., private bug reports), especially in closed-source LLMs may raise privacy and IP concerns.

\textbf{Clear info:} 
\begin{itemize}
  \item Number of repetitions (``When evaluating model performance, it is recommended to conduct multiple tests and average the results.'' \url{https://huggingface.co/deepseek-ai/DeepSeek-R1\#usage-recommendations}), how were repetitions aggregated?, discuss limitations and mitigations
  \item Model version, fixed random seeds
  \item etc...
\end{itemize}

\todo{Discuss what makes the results of a presented study generalizable and why they are not model-dependent. Argue why the results will likely hold for a different (future) model or the next release of the LLM-based tool that was studied (e.g., ChatGPT)?}

\todo{Discuss aspects as such increased performance (see benchmarking guidelines) vs. increased resource consumption and non-determinism.}

\todo{Connect guideline to study types and for each type have bullet point lists with information that MUST, SHOULD, or MAY be reported (usage of those terms according to RFC 2119~\cite{rfc2119}).}

\todo{Balance of evaluation dataset: ``We have chosen our test samples to be small enough to perform manual semantic analysis, but large enough to draw conclusions''~\cite{tinnessoftware}}


\subsubsection{Example(s)}
\textbf{Data Leakage}:
Since much research in software engineering evolves around code, inter-dataset code duplication has been extensively researched and addressed over the years to curate deduplicated datasets (e.g. by Lopes in 2017 ~\cite{10.1145/3133908}, Allamanis in 2019 ~\cite{10.1145/3359591.3359735}, or Lopez in 2025 ~\cite{inter-dataset-lopez2025}).
The issue of inter-dataset duplication has also attracted interest in other disciplines, with growing demands for data mining. For example in the biology field, Lakiotaki et al. ~\cite{10.1093/database/bay011} acknowledge and address the overlap between multiple commonly disease datasets. 
When employing Falcon LLMs the issue of inter-dataset duplication can be partially assessed and mitigated. The technology innovation institute publicly provides access to parts of its training data for the Falcon LLM models ~\cite{technology_innovation_institute_2023} via Huggingface.


\todo{write paragraph}


\subsubsection{Advantages}
\textbf{Data Leakage}:
Assessing and mitigating the effects of inter-dataset duplication strengthens a studies' validity and reliability, as it prevents overly optimistic performance estimates that do not apply to previously unkown samples.

\todo{write paragraph}


\subsubsection{Challenges}
\textbf{Data Leakage}:
Most LLM providers do not publicly offer information about the datasets employed for pre-training, impeding the assessment of inter-dataset duplication effects.

\todo{write paragraph}


\subsubsection{Study Types}

\todo{Connect guideline to study types and for each type have bullet point lists with information that MUST, SHOULD, or MAY be reported (usage of those terms according to RFC 2119~\cite{rfc2119}).}


\subsubsection{References}

\bibliographystyle{plain}
\bibliography{../../literature.bib}

\end{document}

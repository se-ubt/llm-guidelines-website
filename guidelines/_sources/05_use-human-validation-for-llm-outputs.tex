\input{../../header.tex}

\begin{document}

\subsection{Use Human Validation for LLM Outputs}

\subsubsection{Recommendations}

\todo{Sebastian: Revise the recommendations to make clear what we recommend using \must, \should, etc.. Also indicate which information we expect to be reported in the \paper or in the \supplementarymaterial. Note the general comments in the introduction of the guidelines: \url{https://llm-guidelines.org/guidelines\#guidelines} Some content in this subsection reads more like examples, advantages, or challenges. Please revise this as well.}

While LLMs can automate many tasks in research and software development that have traditionally been executed by humans, it is important to validate LLM outputs with human judgment.
For example, in natural language processing tasks, a large-scale study has shown that LLMs have significant variation in their results, which limits their reliability as a direct substitute for human judges~\cite{DBLP:journals/corr/abs-2406-18403}. 
Human validation helps ensure the accuracy and reliability of the results, as LLMs may sometimes produce incorrect or biased outputs.

In studies where LLMs are used to support researchers, human validation \must be done to ensure validity~\cite{DBLP:conf/chi/Wang0RMM24}.
Researchers \should plan for validation in humans from the outset and develop their methodology with this validation in mind.
Researchers \should use established reference models for comparing humans with LLMs~\cite{Schneider2025ReferenceModel}.

In some cases, a hybrid approach between human and machine-generated annotations can improve annotation efficiency.
However, researchers \should use systematic approaches to decide how human annotations can be replaced with LLM-generated ones, such as the methods proposed by Ahmed et al.~\cite{DBLP:journals/corr/abs-2408-05534}.

When conducting empirical measurements of human validation, researchers \must clearly define the construct that they are measuring and \must specify the methods used for measurement. 
Further, they should use established measurement methods and instruments that are empirically validated~\cite{DBLP:journals/fcomp/HoffmanMKL23, DBLP:conf/chi/PerrigSB23}.
Measuring a construct \may require aggregating input from multiple subjects. 
For example, a study might assess inter-rater agreement of humans and LLMs using measures such as Cohen's Kappa or Krippendorff's Alpha before aggregating ratings.

Researchers \should employ human validation of LLM outputs whenever there either is no established reference dataset or benchmark or when automated metrics alone cannot fully capture the qualities they are concerned with.
These cases my include studies such as user-facing interactive studies (e.g., debugging or collaborative coding agents), auditing bias or fairness, controlled experiments on developer productivity or quality (e.g., usefulness evaluation of LLM based IDE plugin).

In some cases, researchers can also combine multiple measures into single composite measures.
As an example, they may evaluate both the time taken and accuracy when completing a task and aggregate them into a composite measure for the participants' overall performance.
In these cases, researchers \should clearly describe their method of aggregation and document their reasoning for doing so.

When employing human validation, additional confounding factors \should be controlled for, such as the level of expertise or experience with LLM-based applications or their general attitude towards AI-based tools.
Researchers should control for these factors through methods such as stratified sampling or by categorizing participants based on experience levels.
Where applicable, researchers \should conduct a power analysis to estimate the required sample size and ensure sufficient statistical power in their experiment design.
When multiple humans are annotating the same artifact, researchers should validate the objectivity of annotations through measures of inter-rater reliability.

\subsubsection{Example(s)}

For instance, ``A subset of 20\% of the LLM-generated annotations was reviewed and validated by experienced software engineers to ensure accuracy. Using Cohen's Kappa, an inter-rater reliability of $\kappa = 0.90$ was reached.'' \todo{citation?}

For instance, ``Model-model agreement is high, for all criteria, especially for the three large models (GPT-4, Gemini, and Claude). Table I indicates that the mean Krippendorffâ€™s $\alpha$ is 0.68-0.76. 
Second, we see that human-model and human-human agreements are in similar ranges, 0.24-0.40 and 0.21-0.48
for the first three categories.''~\cite{DBLP:journals/corr/abs-2408-05534}.

Xue et al.~\cite{DBLP:conf/icse/XueCBTH24} conducted a controlled experiment in which they evaluated the impact of ChatGPT on the performance and perceptions of students in an introductory programming course.
They employed multiple measures to judge the impact of the LLM from the perspective of humans.
In their study, they recorded the students' screens, evaluated the answers they provided in tasks, and distributed a post-study survey to get direct opinions from the students.

Hymel et al.~\cite{hymel2025analysisllmsvshuman} evaluated the capability of ChatGPT-4.0 to generate requirements documents. 
Specifically, they evaluated two requirements documents based on the same business use case, one document generated with the LLM and one document created by a human expert.
The documents were then reviewed by experts and judged in terms of alignment with the original business use case, requirements quality and whether they believed it was created by a human or an LLM.
Finally, they analyzed the influence of the participants' familiarity with AI tools on the study results.
\todo{neil: add the details of how they measured this}

\subsubsection{Advantages}

Incorporating human judgment in the evaluation process adds a layer of quality control and increases the trustworthiness of the study's findings, especially when explicitly reporting inter-rater reliability metrics~\cite{khraisha2024canlargelanguagemodelshumans}.

Incorporating feedback from individuals from the target population strengthens external validity by grounding study findings in real-world usage scenarios and may positively impact the transfer of study results to practice.
Researchers may uncover additional opportunities to further improve the LLM or LLM-based tool based on the reported experiences.

\subsubsection{Challenges}

Measurement through human validation can be challenging.
Ensuring that the operationalization of a desired construct and the method of measuring it are appropriate requires a good understanding of the studied concept and construct validity in general, and a systematic design approach for the measurement instruments~\cite{DBLP:journals/tse/SjobergB23}.

Human judgment is often very subjective and may lead to large variability between different subjects due to differences in expertise, interpretation, and biases among evaluators~\cite{DBLP:journals/pacmhci/McDonaldSF19}.
Controlling for this subjectivity will require additional rigor when analyzing the study results.
Different individuals might have different expectations and experiences that might impact the results of human evaluation. For example, "the struggle with adapting to AI-assisted work is more common for racially minoritized developers. That group also rated the quality of the output of AI-assisted coding tools significantly lower than other groups~\cite{hicks_lee_foster-marks_2025}".

Recruiting participants as human validators will always incur additional resources compared to machine-generated measures.
Researchers must weigh the cost and time investment incurred by the recruitment process against the potential benefits for the validity of their study results.

\subsubsection{Study Types}

\todo{Sebastian: This section rather reads like the actual recommendations. The intention of this section was rather to focus on the study types where the guidelines is most important. For example, when evaluating an LLM-based tool for program repair, with existing benchmarks and non-LLM-baselines, why do I need human validation?}
\todo{Neil: tried to fix this ..}

For \href{/study-types/#studying-llm-usage-in-software-engineering}{Studying LLM Usage in Software Engineering}, researchers \should consider the validity of their evaluation criteria.
In studies where there exist arguably objective evaluation criteria, such as \href{/study-types/#benchmarking-llms-for-software-engineering-tasks}{Benchmarking LLMs for Software Engineering Tasks}, there is little need for human validation, as the benchmark is (hopefully) already validated.

Where criteria are more unclear, such as \href{/study-types/#llms-as-annotators}{LLMs as Annotators}, \href{/study-types/#llms-as-subjects}{LLMs as Subjects}, or \href{/study-types/#llms-for-synthesis}{LLMs for Synthesis}, human validation is important, and these guidelines apply. When using \href{/study-types/#llms-as-judges}{LLMs as Judges}, it is common to have a human first co-create the rating criteria. 

In developing \href{/study-types/#llms-for-new-software-engineering-tools}{LLMs for New Software Engineering Tools}, human validation of the LLM output is critical to ensure the outputs are expected.
When conducting their studies with human validation, researchers

\must
\begin{itemize}
    \item Clearly define the construct measured through human validation.
    \item Describe how the construct is operationalized in the study, specifying the method of measurement.
    \item Employ established and widely accepted measurement methods and instruments.
\end{itemize}

\should
\begin{itemize}
    \item Assess the characteristics of the human validators to control for factors influencing the validation results (e.g., years of experience, familiarity with the task, etc.)
    \item Use empirically validated measures.
    \item Complement automated or machine-generated measures with human validation where possible.
    \item Should ensure consistency among human validators by establishing shared understanding in training sessions or pilot studies and by assessing inter-rater agreement.
\end{itemize}

\may
\begin{itemize}
    \item Use multiple different measures (e.g., expert ratings, surveys, task performance) for human validation.
\end{itemize}

\subsubsection{References}

\bibliographystyle{plainnat}
\bibliography{../../literature.bib}

\end{document}

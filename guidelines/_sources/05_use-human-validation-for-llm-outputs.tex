\input{../../header.tex}

\begin{document}

\subsection{Use Human Validation for LLM Outputs}

\subsubsection{Recommendations}

While LLMs can automate many tasks in research and software development that have traditionally been executed by humans, it is important to validate LLM outputs with human judgment.
For example, in natural language processing tasks, a large-scale study has shown that LLMs have significant variation in their results, which limits their reliability as a direct substitute for human judges~\cite{DBLP:journals/corr/abs-2406-18403}. 
Moreover, LLMs fail to match human annotators in labeling tasks for natural language inference, stance detection, semantic change, and hate speech detection~\cite{DBLP:conf/chi/Wang0RMM24}.
Human validation helps ensure the accuracy and reliability of the results, as LLMs may sometimes produce incorrect or biased outputs.

Researchers \should employ human validation of LLM outputs whenever there either is no established reference dataset or benchmark or when automated metrics alone cannot fully capture the qualities they are concerned with.
These cases may include studies such as user-facing interactive studies (e.g., debugging or collaborative coding agents), auditing bias or fairness, controlled experiments on developer productivity or quality (e.g., usefulness evaluation of LLM based IDE plugin).
In annotation tasks where LLMs have shown high accuracy, researchers \may consider employing LLMs to label a majority of the study samples to improve annotation efficiency.
However, researchers \should use systematic approaches to decide how human annotations can be replaced with LLM-generated ones, such as the methods proposed by Ahmed et al.~\cite{DBLP:journals/corr/abs-2408-05534}.
Their suggested method involves the use of a jury of multiple LLMs, where the model to model agreement is determined for a selection of a smaller set of annotation tasks.
Based on the level of agreement between the LLMs, more or less human ratings are replaced with LLM ratings.

Researchers \should plan for validation with humans from the outset and develop their methodology with this validation in mind.
Researchers \should use established reference models for comparing humans with LLMs.
For example, the reference model by Schneider et al.~\cite{Schneider2025ReferenceModel} provides researchers with an overview describing the groups, activities, and artifacts involved in studies comparing LLMs with humans.
Further, they illustrate the relationship between these entitities, the order of required steps in the study design, and which design decisions should be documented.

When conducting empirical measurements of human validation, researchers \must clearly define the construct that they are measuring and \must specify the methods used for measurement in their \paper.
Further, they \should use established measurement methods and instruments that are empirically validated~\cite{DBLP:journals/fcomp/HoffmanMKL23, DBLP:conf/chi/PerrigSB23}.
When creating prepared instruments to instruct human participants in validating LLM outputs (e.g., questionnaires, study tasks), researchers \should share the instruments in their \supplementarymaterial.

Validating the output of an LLM \may involve the aggregation of inputs from multiple human subjects.
In these cases, researchers \should clearly describe their method of aggregation and document their reasoning for doing so.
When multiple humans are annotating the same artifact, researchers \should validate the objectivity of annotations through measures of inter-rater reliability such as Cohen's Kappa or Krippendorff's Alpha.
When employing human validation, additional confounding factors \should be controlled for, such as the experience or expertise in the relevant domain.
Researchers should control for these factors through methods such as stratified sampling or by categorizing participants based on experience levels.
Where applicable, researchers \should conduct a power analysis to estimate the required sample size and ensure sufficient statistical power in their experiment design.

In summary, when considering whether to employ human validation, researchers
\begin{itemize}
    \item \should employ human validation of LLM outputs whenever there either is no established reference dataset, benchmark or when automated metrics alone cannot fully capture the qualities they are concerned with.
    \item \may consider employing LLMs to label a majority of the study samples if LLMs have shown high annotation accuracy.
    \item \should use systematic approaches to decide how human annotations can be replaced with LLM-generated ones.
\end{itemize}

When designing their methodology, researchers
\begin{itemize}
    \item \should plan for validation with humans from the outset and develop their methodology with this validation in mind.
    \item \should use established reference models for comparing humans with LLMs.
    \item \should use established measurement methods and instruments that are empirically validated.
    \item \may aggregate inputs from multiple human subjects.
    \item \should validate the objectivity of multiple validators through measures of inter-rater reliability such as Cohen's Kappa or Krippendorff's Alpha.
    \item \should control for human confounding factors (e.g., experience/expertise in the relevant domain). 
    \item \should conduct a power analysis to estimate the required sample size and ensure sufficient statistical power in their experiment design.
\end{itemize}

When reporting the results of the human validation, researchers
\begin{itemize}
    \item \must clearly define the construct that they are measuring in their \paper.
    \item \must specify the methods used for human validation in their \paper.
    \item \should include any measurement instruments (e.g., questionnaires, study tasks) in their \supplementarymaterial.
    \item \should clearly describe their method of aggregation and document their reasoning for doing so in their \paper when aggregating measures from multiple subjects.
\end{itemize}

\subsubsection{Example(s)}

For instance, ``A subset of 20\% of the LLM-generated annotations was reviewed and validated by experienced software engineers to ensure accuracy. Using Cohen's Kappa, an inter-rater reliability of $\kappa = 0.90$ was reached.'' \todo{citation?}

For instance, ``Model-model agreement is high, for all criteria, especially for the three large models (GPT-4, Gemini, and Claude). Table I indicates that the mean Krippendorffâ€™s $\alpha$ is 0.68-0.76. 
Second, we see that human-model and human-human agreements are in similar ranges, 0.24-0.40 and 0.21-0.48
for the first three categories.''~\cite{DBLP:journals/corr/abs-2408-05534}.

Xue et al.~\cite{DBLP:conf/icse/XueCBTH24} conducted a controlled experiment in which they evaluated the impact of ChatGPT on the performance and perceptions of students in an introductory programming course.
They employed multiple measures to judge the impact of the LLM from the perspective of humans.
In their study, they recorded the students' screens, evaluated the answers they provided in tasks, and distributed a post-study survey to get direct opinions from the students.

Hymel et al.~\cite{hymel2025analysisllmsvshuman} evaluated the capability of ChatGPT-4.0 to generate requirements documents. 
Specifically, they evaluated two requirements documents based on the same business use case, one document generated with the LLM and one document created by a human expert.
The documents were then reviewed by experts and judged in terms of alignment with the original business use case (scale of 1-10), requirements completion (Not Complete, Fairly Complete, Fully Complete) and whether they believed it was created by a human or an LLM.
Finally, they analyzed the influence of the participants' familiarity with AI tools on the study results.
Participants self-reported their AI tool proficiency (Novice, Intermediate, Advanced, Expert) and usage frequency (Daily, Weekly, Monthly, Never), which were then correlated with their ratings of the requirements documents.


\subsubsection{Advantages}

Incorporating human judgment in the evaluation process adds a layer of quality control and increases the trustworthiness of the study's findings, especially when explicitly reporting inter-rater reliability metrics~\cite{khraisha2024canlargelanguagemodelshumans}.

Incorporating feedback from individuals from the target population strengthens external validity by grounding study findings in real-world usage scenarios and may positively impact the transfer of study results to practice.
Researchers may uncover additional opportunities to further improve the LLM or LLM-based tool based on the reported experiences.

\subsubsection{Challenges}

Measurement through human validation can be challenging.
Ensuring that the operationalization of a desired construct and the method of measuring it are appropriate requires a good understanding of the studied concept and construct validity in general, and a systematic design approach for the measurement instruments~\cite{DBLP:journals/tse/SjobergB23}.

Human judgment is often very subjective and may lead to large variability between different subjects due to differences in expertise, interpretation, and biases among evaluators~\cite{DBLP:journals/pacmhci/McDonaldSF19}.
Controlling for this subjectivity will require additional rigor when analyzing the study results.
Different individuals might have different expectations and experiences that might impact the results of human evaluation. For example, \enquote{the struggle with adapting to AI-assisted work is more common for racially minoritized developers. That group also rated the quality of the output of AI-assisted coding tools significantly lower than other groups}~\cite{hicks_lee_foster-marks_2025}.

Recruiting participants as human validators will always incur additional resources compared to machine-generated measures.
Researchers must weigh the cost and time investment incurred by the recruitment process against the potential benefits for the validity of their study results.

\subsubsection{Study Types}

For \href{/study-types/#studying-llm-usage-in-software-engineering}{Studying LLM Usage in Software Engineering}, researchers \should consider the validity of their evaluation criteria.
In studies where there exist arguably objective evaluation criteria, such as \href{/study-types/#benchmarking-llms-for-software-engineering-tasks}{Benchmarking LLMs for Software Engineering Tasks}, there is little need for human validation, as the benchmark is (hopefully) already validated.

Where criteria are more unclear, such as \href{/study-types/#llms-as-annotators}{LLMs as Annotators}, \href{/study-types/#llms-as-subjects}{LLMs as Subjects}, or \href{/study-types/#llms-for-synthesis}{LLMs for Synthesis}, human validation is important, and these guidelines apply. When using \href{/study-types/#llms-as-judges}{LLMs as Judges}, it is common to have a human first co-create the rating criteria. 

In developing \href{/study-types/#llms-for-new-software-engineering-tools}{LLMs for New Software Engineering Tools}, human validation of the LLM output is critical to ensure the outputs are expected.
Researchers \should make sure to follow the guidelines outlined in this section when employing human validation.

\subsubsection{References}

\bibliographystyle{plainnat}
\bibliography{../../literature.bib}

\end{document}

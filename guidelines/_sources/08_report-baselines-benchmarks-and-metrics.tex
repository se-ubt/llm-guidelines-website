\input{../../header.tex}

\begin{document}

\subsection{Report Suitable Baselines, Benchmarks, and Metrics}

\subsubsection{Recommendations}

\todo{indicate what information is supposed to be reported in the \paper or in the \supplementarymaterial}

\todo{double-check usage of \must, \should, etc.}

Empirical software engineering showed the importance of validating tools supporting software engineering \cite{DBLP:conf/icse/PerryPV00, DBLP:conf/ease/Hasselbring21}.
Thus, it is of pivotal importance to empirically assess the effectiveness of LLMs or LLM-based tools.
%Benchmarks, baselines, and metrics play a significant role in assessing the effectiveness of a model.
%Effectively solving software engineering tasks requires evaluating how well the given model addresses a specific problem.
Benchmarks are model- and tool-independent standardized tests used to assess the performance of LLMs on specific tasks such as code summarization or code generation.
A benchmark consists of multiple standardized test cases, each with at least a task and an expected result.
Metrics are used to quantify the performance for the benchmark tasks, enabling a comparison.
%Metrics capture a single aspect of the model's quality and enable comparing diverse models.
%Researchers can pick the best model for the given task based on the metrics.
Since LLMs require substantial hardware resources, baselines serve as a comparison to assess their performance against traditional algorithms with lower computational costs.
%A baseline represents a reference point for the measured LLM.
%The following section describes benchmarks, metrics, and baselines used.
%For each, we provide examples, recommendations, and challenges researchers should be aware of.
When selecting benchmarks, it is important to understand both the contained task to solve and the expected result because this determines what the benchmark assesses.
We recommend that researchers briefly summarize the selected benchmark and why it is suitable for their study.
They should report why the given tasks and corresponding expected benchmark results reflect the problem the researcher wants to solve.
In addition, reporting the total number of unique benchmark test cases and illustrating an example of a single test case allows other researchers to assess what the model is tested on.
If multiple benchmark exist for the same task, the goal should be to compare performance between benchmarks.
We recommend the use of the most specific benchmarks given the context.

Furthermore, the representativeness of a benchmark is important to report.
For example, many benchmarks focus heavily on Python, and often on isolated functions.
This assesses a very specific part of software development, which is certainly not representative for the full breadth of software engineering.

The use of LLMs might not always be justifiable if traditional approaches achieve similar performance. 
For many tasks LLMs are being evaluated for, there exist traditional non-LLM-based approaches (e.g., for program repair) that can serve as a baseline.
Even if LLM-based tools perform better, the question is whether the resources consumed justify the potentially marginal improvements.
We recommend researchers to always check whether such traditional baselines exist and if they do, compare them with the LLM or LLM-based tool using suitable metrics.
To make such comparisons between traditional and LLM-based approaches, or comparison between LLM-based tools based on a benchmark for the given context.
In general, we recommend to use established metrics whenever possible (see summary below), as this enables secondary research.
We further recommend researchers to carefully argue why the selected metrics are suitable for the given task or study. 
If an LLM-based tool that is supposed to support humans is evaluated, a relevant metric might be the acceptance rate, meaning the ratio of all accepted artifacts (e.g., test cases, code snippets) in relation to all artifacts that were generated and presented to the user.
Another way of evaluating LLM-based tools is calculating inter-model agreement (see also Section \href{/guidelines/#use-an-open-llm-as-a-baseline}{Use an Open LLM as a Baseline}).
This also allows researchers to assess how dependent a tool's performance is on specific models.

LLM-based generation is non-deterministic by-design.
This non-determinism requires the repetition of experiments to statistically assess the performance of a model or tool using the arithmetic mean, confidence intervals, and standard deviations.

\subsubsection{Example(s)}

Two benchmarks used for code generation are \emph{HumanEval} (\href{https://github.com/openai/human-eval}{GitHub}) \cite{DBLP:conf/acl/PapineniRWZ02} and \emph{MBPP} (\href{https://huggingface.co/datasets/google-research-datasets/mbpp}{GitHub}) \cite{DBLP:journals/corr/abs-2108-07732}.
Both benchmarks consist of code snippets written in Python sourced from publicly available repositories.
Each snippet consists of four parts: a prompt based on function definition and a corresponding description what the function should accomplish, a canonical solution, an entry point for execution, and tests.
The input of the LLM is the entire prompt.
The output of the LLM is evaluated either against the canonical solution using metrics or against a test suite.
Other benchmarks for code generation include \emph{ClassEval} (\href{https://github.com/openai/human-eval}{GitHub}) \cite{DBLP:journals/corr/abs-2308-01861}, \emph{LiveCodeBench} (\href{https://github.com/LiveCodeBench/LiveCodeBench}{GitHub}) \cite{DBLP:journals/corr/abs-2403-07974}, and \emph{SWE-bench} (\href{https://github.com/swe-bench/SWE-bench}{GitHub}) \cite{DBLP:conf/iclr/JimenezYWYPPN24}.
An example of a code translation benchmark is \emph{TransCoder}~\cite{DBLP:journals/corr/abs-2006-03511} (\href{https://github.com/facebookresearch/CodeGen}{GitHub}). 

According to \citet{10.1145/3695988}, main problems types for LLMs are classification, recommendation and generation problems.
Each of these problem types requires a different set of metrics.
They provide a comprehensive overview of benchmarks categorized by software engineering tasks.
Common metrics for assessing generation tasks are \emph{BLEU}, \emph{pass@k}, \emph{Accuracy/ Accuracy@k}, and \emph{Exact Match}~\cite{10.1145/3695988}.
The most common recommendation task metric is \emph{Mean Reciprocal Rank}~\cite{10.1145/3695988}.
For classification tasks, classical machine learning metrics such as \emph{Precision}, \emph{Recall}, \emph{F1-score}, and \emph{Accuracy} are often reported \cite{10.1145/3695988}.

We now briefly discuss two common metrics used for generation tasks.
\emph{BLEU-N} \cite{DBLP:conf/acl/PapineniRWZ02} is a similarity score based on n-gram precision between two strings, ranging from $0$ to $1$.
Values close to $0$ depict dissimilar values closer to $1$ represent similar content.
A value closer to $1$ indicates that the model is more capable of generating the expected output for code generation.
%The score measures how many n-grams from the generated output of a model appear in the target string.
%In isolation, this approach favors shorter over longer generated sequences because it is more likely to have fewer n-grams in an extensive target sequence than more.
%To mitigate this, the so-called brevity penalty is introduced to disincentivize short sequences when having longer target sequences.
\emph{BLEU-N} has multiple variations.
\emph{CodeBLEU} \cite{DBLP:journals/corr/abs-2009-10297} and \emph{CrystalBLEU} \cite{DBLP:conf/kbse/EghbaliP22} are the most notable variations tailored to code, by introducing additional heuristics such as AST matching.
As mentioned above, researchers should motivate why they chose a certain metric or variant thereof for their particular study.

The metric \emph{pass@k} reports the likelihood of a model correctly completing a code snippet at least once within \emph{k} tries.
To the best of our knowledge, the basic concept of pass@k was first used in \cite{DBLP:journals/corr/abs-1906-04908} for evaluating code synthesis under the name \emph{success rate at B}, where B denotes the budget of trials.
The term pass@k was later popularized by \cite{DBLP:journals/corr/abs-2107-03374} as a metric for code generation correctness.
The exact definition of correctness varies depending on the task
For code generation, correctness is often defined based on test cases. A passing test then means that the solution is correct.
The resulting pass rate ranges from $0$ to $1$.
A pass rate of $0$ indicates that the model was not able to generate a single correct solution within $k$ tries.
A pass rate of $1$ indicates that the model successfully generated at least one correct solution in $k$ tries.
The metric is defined as:

$\text{pass@k} = 1 - \frac{\binom{n-c}{k}}{\binom{n}{k}}$

Where $n$ is the total number of generated samples per prompt, $c$ is the number of correct samples among  $n$, and $k$ is the number of samples.

Choosing an appropriate value for \emph{k} depends on the downstream task of the model and how end-users interact with the model.
% k= 1 -- pass@1
A high pass rate for \emph{pass@1} is highly desirable in tasks where the system only presents one solution or if a single solution requires high computational effort.
For example, code completion depends on a single prediction since the end user typically sees only a single suggestion.
%In automated code refactoring, a large context might lead to high computational costs, making the calculation of a single solution expensive.
%Thus, having a high pass rate at pass@1 is crucial for such tasks.
% k = >=2 -- pass@{>=2}
Pass rates for higher $k$ values (e.g., $2$, $5$, $10$) indicate whether the model can solve the given task within multiple attempts.
For downstream tasks that permit multiple solutions or user interaction, strong performance at k > 1 can be justified. 
For instance, a user selecting the correct test case from multiple suggestions allows for some model errors.
%In code search, finding the most appropriate good snippet for a given context, a reranking algorithm mitigate low pass rates at low k-values.

%\subsubsection{Exact Match}

%As the name suggests, the metric \emph{Exact Match} calculates the percentage of replicas a model can produce.
%If the model is able to produce the exact target sequence a score of 1 is awarded; otherwise 0.
%Compared to BLEU-N, \emph{Exact Match} is a stricter measurement.
%Due to its strictness, reported scores are usually low.

%Code snippets provided by benchmarks contain untrusted code, making sandboxing recommendable.
%To avoid potential security risks, execute untrusted code in an isolated environment, such as a virtual machine or containerization, instead of your own environment.

Common examples for papers using pass@k are papers introducing new models for code generation such as \cite{DBLP:journals/corr/abs-2308-12950, DBLP:journals/corr/abs-2401-14196, DBLP:journals/corr/abs-2409-12186, DBLP:journals/corr/abs-2305-06161}.
%\todo{Add SE papers that use some of the benchmarks and metrics discussed above}
\todo{Find SE papers which uses other benchmarks than pass@k \& other papers which do not introduce a new model.}

\subsubsection{Advantages}

% Provide comparability
% Provide reproducability
% Track progress
% leaderboards
% Tool to select the best model for a given task

\subsubsection{Challenges}

A general challenge with benchmarks for LLMs is that the most prominent ones, such as \emph{HumanEval} and \emph{MBPP}, use Python, introducing a bias towards this specific programming language and its idiosyncrasies.
Since model performance is measured against these benchmarks, researchers often optimize for them.
As a result, performance may degrade if programming languages other than Python are used.

Many closed-source models, such as those released by OpenAI, achieve exceptional performance on certain tasks but lack transparency and reproducibility~\cite{DBLP:conf/nips/00110ZZDJLHL24, DBLP:journals/corr/abs-2308-01861, DBLP:journals/corr/abs-2406-15877}.
Benchmark leaderboards, particularly for code generation, are led by close-sourced models~\cite{DBLP:journals/corr/abs-2308-01861, DBLP:journals/corr/abs-2406-15877}.
While researchers should compare performance against these models, they must consider that providers might discontinue them or apply undisclosed pre- or post-processing beyond the researcher's control (see also Section see also Section \href{/guidelines/#use-an-open-llm-as-a-baseline}{Use an Open LLM as a Baseline}).
%The lack of transparency introduces challenges in analyzing mistakes, limiting the understanding of model failures.
%Open-source models offer greater transparency since the entire process is under the researcher's control; however, they require appropriate hardware.

Challenges with individual metrics include that, for example, \emph{BLEU-N} is a syntactic metric and hence does not measure semantic correctness or structural correctness.
Thus, a high \emph{BLEU-N} score does not directly indicate that the generated code is executable.
%\todo{Mention that alternatives exist? Do they come with other downsides?}
While alternatives exist, they often come with their own limitations.
For instance, \emph{Exact Match} is a strict measurement that does not account for functional equivalence but syntactically different code.
Execution-based metrics (e.g. pass@k) directly evaluate correctness by running test cases, but they require a setup with an execution environment.
When researchers observe unexpected values for certain metrics, the specific results should be investigated in more detail to uncover potential problems.
These problems can, for example, be related to formatting since code formatting highly influences metrics such as \emph{BLEU-N} or \emph{Exact Match}.

Another challenge to consider is that metrics usually capture one specific aspect of a task or solution.
For instance, metrics such as \emph{pass@k} do not reflect qualitative aspects of code such as maintainability, cognitive load, or readability.
These aspects are critical for the downstream task and influence the overall usability.
Moreover, benchmarks are isolated test sets and may not fully represent real-world applications.
For example, benchmarks such as \emph{HumanEval} synthesize code based on written specifications.
However, such explicit descriptions are rare in real-world applications.
Thus, evaluating the model performance with benchmarks might not reflect real-world tasks and end-user usability.

Finally, benchmark data contamination~\cite{DBLP:journals/corr/abs-2406-04244} continues to be a major challenge as well.
In many cases, the training data set for an LLM is not released in conjunction with the model.
The benchmark itself could be part of the model's training dataset.
Such benchmark contamination may lead to the model remembering the actual solution from the training data rather than solving the new task based on the seen data.
This leads to artificially high performance on the benchmark.
For unforeseen scenarios, however, the model might perform much worse.

%Models may become overfitted to the training data, leading to poor performance on unseen data.
%When optimizing for a metric, the underlying model might adjust its parameters to improve the metric, thereby failing to generalize.
%This can result in a model performing exceptionally well on a benchmark but struggling to hold up the performance in unforeseen settings, such as real-world scenarios.


\subsubsection{Study Types}

\todo{Connect guideline to study types and for each type have bullet point lists with information that MUST, SHOULD, or MAY be reported (usage of those terms according to RFC 2119~\cite{rfc2119}).}

This guideline must be followed for all study types that evaluate the performance of plain LLMs on a given task.

For example, if you use an LLM as an annotator and the research goal is to assess which model annotates best based on a test dataset, you \must report an appropriate metric that reflects the nature of your task and you \should disclose the used dataset for evaluation.
Annotation tasks can vary significantly: Are multiple labels allowed for the same sequence? Are the available labels predefined, or should the LLM generate a set of labels independently?
Due to this task dependence, you \should justify your choice of metric, explaining what aspects of the task it captures and what its limitations are.

If you are conducting a well-established task, such as code generation, you \should report standard metrics like pass@k and compare to other models.

\todo{Describe more study types. Copy common outline for this section.}

\subsubsection{References}

\bibliographystyle{plainnat}
\bibliography{../../literature.bib}

\end{document}


%% * [pass@k](https://arxiv.org/pdf/2107.03374) Pass@1 Pass@2 Pass@5 Pass@10, reporting of k depends on the specific usecase of the tool. e.g. for code completion in an IDE pass@1 is essential because the user only get to see one result, pass@5 and pass@10 show that LLMs are somewhat capable of solving the given task (indication that through e.g. fine-tuning it might be possible to solve) or if users get multiple options to choose from (e.g. test generation)
%% * [BLEU-N](https://aclanthology.org/P02-1040.pdf)
%% * [CodeBLEU](https://arxiv.org/abs/2009.10297)
%% * [CrystalBLEU](https://software-lab.org/publications/ase2022_CrystalBLEU.pdf)
%% * [ROUGE](https://aclanthology.org/W04-1013.pdf)
%% * Accuracy/Accuracy@k
%% * Mean Reciprocal Rank
%% * Edit Similarity (ES)
%% * Edit Distance (ED)
%% * Exact Match (EM)
%% * [METEOR](https://dl.acm.org/doi/pdf/10.5555/1626355.1626389)
%% * Recall
%% * F1-score
%% * Mean Reciprocal Rank (MRR)
%% * Mean Average Ranking (MAR)
%% * Mean First Ranking (MFR)
%% * [Character n-gram F-Score (ChrF)](https://aclanthology.org/W15-3049.pdf)
%% * [CodeBERTScore](https://arxiv.org/pdf/2302.05527)
%% * Perplexity (PP)


%% #### Domain-specific metrics
%%
%% * If a tool is analyzed, the acceptance rate of generated artifacts could be interesting (how many artifacts were accepted/rejected by the user)
%% * Inter-model-agreement (related to section on open LLM as baseline): Ask different LLMs or differently confidered LLMs and determine their agreement
%% * Cross Cutting Metrics: Costs/Energy requirements to get the results (database community often reports on how expensive it was to get results, would be interesting for LLMs too but nobody does it yet)

%% #### Classification

%% - Precision
%% - Recall
%% - F1-Score
%% - Accuracy
%% - Area Under the Receiver Operating Characteristic (ROC) Curve
%% - Receiver Operating Characteristic (ROC)
%% - False Positive Rate (FPR)
%% - False Negative Rate (FNR)
%% - Matthews Correlation Coefficient (MCC)

%% #### Recommendation

%% - Mean Reciprocal Rank (MRR)
%% - Precision/Precision@k
%% - MAP/MAP@k
%% - F-score/F-score@k
%% - Recall/Recall@k
%% - Accuracy

%% ### Benchmarks

%% ***TODO:*** Maybe something along the lines of using different benchmarks? Being aware of their biases (e.g., focus on a particular programming language such as Python)?

%% * [HumanEval](https://github.com/openai/human-eval)
%% * [REPOCOD](https://huggingface.co/datasets/lt-asset/REPOCOD)
%% * [ClassEval](https://arxiv.org/abs/2308.01861)

%% #### Code Translation

%% * [Avatar](https://arxiv.org/pdf/2108.11590)
%% * [Transcoder](https://arxiv.org/pdf/2006.03511)

% #### Baselines

% Provide a reference point for existing research
% - often include traditional non-LLM based
% - used to show a difference between (often) low-cost solutions and
% - should report the same metric e.g.
% Example:
% - LLM for test prioritisation
% - LLM

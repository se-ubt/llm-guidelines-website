\documentclass[11pt]{article}
\usepackage[parfill]{parskip} % use newlines for paragraphs (more similar to Markdown)
\newcommand{\todo}[1]{{\textbf{TODO:}\ \textit{#1}}} % command for TODOs
\usepackage{hyperref}

\begin{document}


<<<<<<< HEAD
<<<<<<< HEAD

\section{Benchmarks, Baselines and Metric}
=======
>>>>>>> 8e2d179 (Added first draft of benchmark+pass@k)
% Introduction
% - [ ] Maybe already mention limitations/ challenges
Benchmarks, baselines, and metrics play a significant role in assessing the effectiveness of a model.
Effectively solving software engineering tasks requires evaluating how well the given model addresses a specific problem.
A benchmark is a set of tests designed to evaluate a model's performance on a given task.
A benchmark consists of predefined, standardized problems along with their expected solutions.
Metrics quantify specific aspects of a model's performance.
Metrics capture a single aspect of the model's quality and enable comparing diverse models.
Researchers can pick the best model for the given task based on the metrics.
Since LLMs require substantial hardware resources, baselines serve as a comparison to assess their performance against traditional algorithms with lower computational costs.
Thus, a baseline represents a reference point for the measured LLM.
The following section describes benchmarks, metrics, and baselines used.
For each, we provide examples, recommendations, and challenges researchers should be aware of.

<<<<<<< HEAD
<<<<<<< HEAD
\subsection{Benchmarks, Baselines and Metrics}
=======
\subsection{Benchmarks}
>>>>>>> 8e2d179 (Added first draft of benchmark+pass@k)
=======
\subsection{Benchmarks, Baselines and Metrics}
>>>>>>> 9b86b4e (Added challenges)

Benchmarks are model-independent standardized tests used to assess the performance of LLMs on specific tasks.
In software engineering, benchmarks for LLMs are tailored for a specific task related to software engineering, such as code completion or code comprehension, allowing researchers to identify weaknesses and track improvements over time.
The most prominent task for LLMs in Software Engineering is code generation.
% Examples benchmark
The prevalent benchmarks used for code generation are HumanEval \cite{TODO} and MBPP \cite{MBPP}.
Both benchmarks consist of code snippets written in Python sourced from publically available repositories.
Each snippet consists of four parts: a prompt based on function definition and a corresponding description what the function should accomplish, a canoncial solution, an entrypoint reference for execution and tests.
The input of the LLM is the entire prompt.
The output of the LLM is evaluated either against the canoncial solution or against the fullilment of all tests using metrics.

<<<<<<< HEAD
<<<<<<< HEAD
\subsection{Metrics}
% Metrics
Metrics capture the quality of a model's performance in a single value.
Choosing a metric highly depends on the given task type at hand.
Based on \cite{10.1145/3695988}, the main problems types for LLMs are classification, recommendation and generation problems.
Each of these problem types require a different set of metrics.

The following description highlights the most frequently used metrics for each category.
In noval settings, other metrics not described below may be more suitable. % TODO But when>

In the context of generation, BLEU, pass@k, Accuracy/ Accuracy@k and Exact Match are often reported.
For recommendation tasks, the most common metric is Mean Reciprocal Rank.
And for classification tasks, classical machine learning metrics such as Precision, Recall, F1-score and Accuracy are reported.

\cite{10.1145/3695988} provides both a comprehensive overview of benchmarks categorized by software engineering tasks and overall used metrics.

\subsubsection{BLEU-N}

BLEU-N \cite{DBLP:conf/acl/PapineniRWZ02} is a similarity score based on n-gram precision between two strings, ranging from 0 to 1.
Values close to 0 depict dissimilar values closer to 1 represent similar content.
A value closer to 1 indicates that the model is more capable of generating the expected output for code generation.
BLEU does not measure semantic correctness nor structural correctness; thus, a high BLEU score does not directly indicate that the generated code is executable.
% Short description of BLEU
The score measures how many n-grams from the generated output of a model appear in the target string.
In isolation, this approach favors shorter over longer generated sequences because it is more likely to have fewer n-grams in an extensive target sequence than more.
To mitigate this, the so-called brevity penalty is introduced to disincentivize short sequences when having longer target sequences.
BLEU has multiple variations.
CodeBLEU \cite{DBLP:journals/corr/abs-2009-10297} and CrystalBLEU \cite{DBLP:conf/kbse/EghbaliP22} are the most notable variations tailored to code, by introducing additional heuristics such as AST matching.

\subsubsection{Pass@k}
% Pass@k Metric
Pass@k reports the likelihood of a model correctly completing a code snippet at least once within \emph{k} tries.
What correctness is depends on the given task, but for code generation it is often whether provided tests pass or not.
The resulting pass rate ranges from 0 to 1.
A pass rate of 0 indicates that the model was not able to generate a single correct solution within k tries.
A pass rate of 1 indicates that the model successfully generated at least one correct solution in k tries.
% Choosing k-value
=======
=======
\subsection{Metrics}
% Metrics
>>>>>>> 9b86b4e (Added challenges)
Metrics capture the quality of a model's performance in a single value.
Choosing a metric highly depends on the given task type at hand.
Based on \cite{10.1145/3695988}, the main problems types for LLMs are classification, recommendation and generation problems.
Each of these problem types require a different set of metrics.

<<<<<<< HEAD
>>>>>>> 8e2d179 (Added first draft of benchmark+pass@k)
=======
The following description highlights the most frequently used metrics for each category.
In noval settings, other metrics not described below may be more suitable. % TODO But when>

In the context of generation, BLEU, pass@k, Accuracy/ Accuracy@k and Exact Match are often reported.
For recommendation tasks, the most common metric is Mean Reciprocal Rank.
And for classification tasks, classical machine learning metrics such as Precision, Recall, F1-score and Accuracy are reported.

\cite{10.1145/3695988} provides both a comprehensive overview of benchmarks categorized by software engineering tasks and overall used metrics.

\subsubsection{BLEU}

BLEU is a similarity score based on n-gram precision between two strings in the range of 0 and 1. %TODO mention brevity penalty?
While values close to 0 depict dissimilar values closer to 1 represent similar content.
For code generation a value closer to 1 indicates that the model is more capable of generating the expected output.
BLEU does not measure semantic correctness, thus a high BLEU score does not directly indicate that the generated code is executable.

BLEU has multipe variations.
The most common variations for used for assessing are CodeBLEU \cite{TODO} and CrystalBLEU \cite{TODO}.

\subsubsection{Pass@k}
% Pass@k Metric
Pass@k reports the likelihood of a model correctly completing a code snippet at least once within \emph{k} tries.
What correctness is depends on the given task, but for code generation it is often whether provided tests pass or not.
The resulting pass rate ranges from 0 to 1.
A pass rate of 0 indicates that the model was not able to generate a single correct solution within k tries.
A pass rate of 1 indicates that the model successfully generated at least one correct solution in k tries.
% Choosing k-value
>>>>>>> 9b86b4e (Added challenges)
\text{Pass@k} = 1 - \frac{\binom{n-c}{k}}{\binom{n}{k}}
\begin{description}
    \item[n:] total number of generated samples per prompt
    \item[c:] number of correct samples among  n
    \item[k:] number of samples
\end{description}

<<<<<<< HEAD
<<<<<<< HEAD
=======
% Pass@k Metric
Pass@k reports the likelihood of a model correctly completing a code snippet at least once within \emph{k} tries.
The resulting pass rate ranges from 0 to 1.
A pass rate of 0 indicates that the model was not able to generate a single correct solution within k tries.
A pass rate of 1 indicates that the model successfully generated at least one correct solution in k tries.
% Choosing k-value
>>>>>>> 8e2d179 (Added first draft of benchmark+pass@k)
=======
>>>>>>> 9b86b4e (Added challenges)
Choosing an appropriate value for \emph{k} depends on the downstream task of the model and how end-users interact with the model.
% k= 1 -- pass@1
A high pass rate at pass@1 is highly desirable in tasks where the system only presents one solution or if a single solution requires high computational effort.
For example, code completion depends on a single prediction since the end user typically sees only a single suggestion.
In automated code refactoring, a large context might lead to high computational costs, making the calculation of a single solution expensive.
Thus, having a high pass rate at pass@1 is? crucial for such tasks.
% k = >=2 -- pass@{>=2}
Pass rates at higher k values (e.g., 2, 5, 10) indicate whether the model can solve the given task if multiple attempts are tolerated.
Human-in-the-loop workflows or selection implementations such as filtering and reranking can tolerate a higher number of attempts.
For example, the end user might select between multiple suggestions in test case generation, making higher k-values acceptable.
In code search, finding the most appropiate good snippet for a given context, a reranking algorithm mitgate low passrates at low k-values.

<<<<<<< HEAD
<<<<<<< HEAD
\subsubsection{Accuracy/ Accuracy@k}
\todo{}

\subsubsection{Exact Match}

As the name suggests, the metric \emph{Exact Match} calculates the percentage of replicas a model can produce.
If the model is able to produce the exact target sequence a score of 1 is awarded; otherwise 0.
Compared to BLEU-N, \emph{Exact Match} is a stricter measurement.
Due to its strictness, reported scores are usually low.

\subsubsection{Mean Reciprocal Rank}

The metric Mean Reciporcal Rank is often used for recommendation tasks.
\todo{}

\subsubsection{Precision, Recall, F1-score and Accuracy}
\todo{}

\subsection{}
\subsubsection{Context}
\subsection{New Stuff}

\subsubsection{Recommendations}

% Investigate what kind of data you have at hand from the benchmark by other reading through the paper of the benchmark or making an evaluation by yourself
Before executing a benchmark, the authors recommend to investigate what kind of data the benchmark uses.
Without knowing the underlying data of a benchmark, argumentating and dicussing the performance of a model becomes impossible.
=======
=======
The edit distance, is also known as Levenshtein distance
=======
\subsubsection{Accuracy/ Accuracy@k}
\todo{}
>>>>>>> 9b86b4e (Added challenges)

\subsubsection{Exact Math}
\todo{}

\subsubsection{Mean Reciprocal Rank}
\todo{}

\subsubsection{Precision, Recall, F1-score and Accuracy}
\todo{}

\subsection{}
<<<<<<< HEAD

<<<<<<< HEAD
>>>>>>> 8e2d179 (Added first draft of benchmark+pass@k)
\subsubsection{Recommendations}

\todo{outline context}

\todo{What are suitable metrics and benchmarks for evaluating LLMs? A good starting point could be this paper~\cite{10.1145/3695988}.}
<<<<<<< HEAD
>>>>>>> fa2adc1 (Revise structure)

% In case benchmarks do not make sense, go one level deeper and investigate why and how a metric is calculated. It might be the case that different problems are present e.g. wrong formation
In case of unexpected scores of metrics, results should be investigated in more detail, to uncover potential problems such as wrong formating.
For example, formating of code highly influences some metric calculation such as BLEU or Exact Match.
=======
=======
=======
>>>>>>> 2f1f7de (Added challenges)
\subsubsection{Context}
\subsection{New Stuff}

% Baselines
% Provide a reference point for existing research
% - often include traditional non-LLM based
% - used to show a difference between (often) low-cost solutions and
% - should report the same metric e.g.
% Example:
% - LLM for test prioritisation
% - LLM

\subsubsection{Recommendations}

% Investigate what kind of data you have at hand from the benchmark by other reading through the paper of the benchmark or making an evaluation by yourself
% In case benchmarks do not make sense, go one level deeper and investigate why and how a metric is calculated. It might be the case that different problems are present e.g. wrong formation

% Execution of untrusted code --> use sandboxing

% Repeat the generation because of non-determinsm
% Use multiple benchmarks to ease the bias.

<<<<<<< HEAD
\textbf{TODO:} What are suitable metrics and benchmarks for evaluating LLMs? A good starting point could be this paper~\cite{10.1145/3695988}.
>>>>>>> c5e0665 (Added first draft of benchmark+pass@k)
=======
% If appropirate use existing techniques such as static code analysis as comparison.
>>>>>>> 2f1f7de (Added challenges)

%\textbf{TODO:} Connect guideline to study types and for each type have bullet point lists with information that MUST, SHOULD, or MAY be reported (usage of those terms according to RFC 2119~\cite{rfc2119}).

% \subsubsection{Example} % Examples are integrated

<<<<<<< HEAD
% Evaluation techniques of benchmarks
The challenge is to find the correct benchmark and metric for the given problem.

### Code Generation

For code generation the following metrics are reported. ***TODO Note: I think we should only explain the top n***

* [pass@k](https://arxiv.org/pdf/2107.03374) Pass@1 Pass@2 Pass@5 Pass@10, reporting of k depends on the specific usecase of the tool. e.g. for code completion in an IDE pass@1 is essential because the user only get to see one result, pass@5 and pass@10 show that LLMs are somewhat capable of solving the given task (indication that through e.g. fine-tuning it might be possible to solve) or if users get multiple options to choose from (e.g. test generation)
* [BLEU-N](https://aclanthology.org/P02-1040.pdf)
* [CodeBLEU](https://arxiv.org/abs/2009.10297)
>>>>>>> 8e2d179 (Added first draft of benchmark+pass@k)

<<<<<<< HEAD
% Execution of untrusted code --> use sandboxing
Code snippets provided by benchmarks contain untrusted code, making sandboxing recommendable.
To avoid potential security risks, execute untrusted code in an isolated environment, such as a virtual machine or containerization, instead of your own environment.

<<<<<<< HEAD
<<<<<<< HEAD
% Repeat the generation because of non-determinsm
LLM generation is by-design non-determinisc making repeated experiments mandatory. % as long as temperature/ top-p / top-k is set correctly
As long as hyperparameters (e.g. temperature, top-p and top-k) are not limiting randomness, researchers should execute experiments multiple times and calculate averages, confidence intervals or standard deviations across multiple runs.
=======
\todo{TODO: Mention aspects such as repeating the generation to handle non-determinism (``When evaluating model performance, it is recommended to conduct multiple tests and average the results.'' https://huggingface.co/deepseek-ai/DeepSeek-R1#usage-recommendations) how were repetitions aggregated?}
>>>>>>> 777ea9f (Update TODO)
=======
\textbf{TODO: Mention aspects such as repeating the generation to handle non-determinism (``When evaluating model performance, it is recommended to conduct multiple tests and average the results.''  \url{https://huggingface.co/deepseek-ai/DeepSeek-R1\#usage-recommendations}) how were repetitions aggregated?}
>>>>>>> ee91890 (Fix minor issues, add new macro for TODOs)
=======
\todo{Maybe something along the lines of using different benchmarks? Being aware of their biases (e.g., focus on a particular programming language such as Python)?}

<<<<<<< HEAD
\todo{Mention aspects such as repeating the generation to handle non-determinism (``When evaluating model performance, it is recommended to conduct multiple tests and average the results.''  \url{https://huggingface.co/deepseek-ai/DeepSeek-R1\#usage-recommendations}) how were repetitions aggregated?}
>>>>>>> fa2adc1 (Revise structure)

<<<<<<< HEAD
% Use multiple benchmarks to ease the bias.
Distinctive benchmarks evaluated on a model strengthen the meaningfulness of statements derived from a metric.
A single benchmark only tests an isolated aspect on a model.
The benchmark could have been leaked in the training data set, sophisticating the benchmark results.
To mitigate this, the authors recommend to test a model on multiple benchmark and report each
=======
\begin{enumerate}
\item HumanEval \url{https://github.com/openai/human-eval}
\item REPOCOD \url{https://huggingface.co/datasets/lt-asset/REPOCOD}
\item CoderEval \url{https://github.com/CoderEval/CoderEval}
\item ...
\end{enumerate}
=======
%\todo{TODO: Mention aspects such as repeating the generation to handle non-determinism}
>>>>>>> c5e0665 (Added first draft of benchmark+pass@k)
>>>>>>> 8e2d179 (Added first draft of benchmark+pass@k)

<<<<<<< HEAD
% If appropirate use existing techniques such as static code analysis as comparison.
If your model supplements traditional tools or approaches, the researcher should compare against them.

%\textbf{TODO:} Connect guideline to study types and for each type have bullet point lists with information that MUST, SHOULD, or MAY be reported (usage of those terms according to RFC 2119~\cite{rfc2119}).


% Reproducibility
Experiment setup including hyperparameters must be reported to ensure reproducibility.

=======
\todo{Connect guideline to study types and for each type have bullet point lists with information that MUST, SHOULD, or MAY be reported (usage of those terms according to RFC 2119~\cite{rfc2119}).}

\todo{In some cases, there might be tools/methods using ``traditional'' approaches (like static analysis) that the LLM-based approach needs to be compared with. This is what is meant by ``baselines'' in the title.}
>>>>>>> fa2adc1 (Revise structure)


<<<<<<< HEAD
\begin{center}
\begin{tabular}{||c c c||}
\hline
Benchmark & Main SE Area & Reference Link % Add language?/ purpose?
\hline\hline

HumanEval \cite{DBLP:conf/acl/PapineniRWZ02} & Code Generation & https://github.com/openai/human-eval \\
\hline

MBPP \cite{DBLP:journals/corr/abs-2108-07732} & Code Generation & https://huggingface.co/datasets/google-research-datasets/mbpp \\
\hline

ClassEval \cite{DBLP:journals/corr/abs-2308-01861} & Code Generation & https://github.com/openai/human-eval \\
\hline

TransCoder \cite{{DBLP:journals/corr/abs-2006-03511} & Code Translation & https://github.com/facebookresearch/CodeGen \\
\hline

\end{tabular}
\end{center}
=======
\subsubsection{Example(s)}
>>>>>>> fa2adc1 (Revise structure)

\todo{write paragraph}
=======
% \subsubsection{Benefits}
>>>>>>> 2f1f7de (Added challenges)


\subsubsection{Advantages}

\todo{write paragraph}

<<<<<<< HEAD
<<<<<<< HEAD
% Provide comparability
% Provide reproducability
% Track progress
% leaderboards
% Tool to select the best model for a given task

\subsubsection{Challenges}

% Bias throughs Python
The most prominent benchmarks for LLMs use Python, introducing a bias towards Python-based programs.
In code generation, HumanEval and MBPP are widely used benchmarks.
Since model performance is measured against these benchmarks, researchers often optimize for them.
As a result, performance may degrade if programming languages other than Python are used.

% Closed source vs open source
Many closed-source models, such as those released by OpenAI, achieve exceptional performance on certain tasks but lack transparency and reproducibility.
Benchmark leaderboards, particularly for code generation, are led by close-sourced models.
While researchers should compare performance against these models, they must consider that providers might discontinue them or apply undisclosed pre- or postprocessing beyond the researcher's control.
The lack of transparency introduces challenges in analyzing mistakes, limiting the understanding of model failures.
Open-source models offer greater transparency since the entire process is under the researcher's control; however, they require appropriate hardware.

% Not all metrics can be captured by fully automated metrics
% Benchmarks might not generalize to real-world tasks / Benchmarks only measure one aspect in an isolated setting
% Fail to capture other qualitative aspects (e.g. readability, maintainability)
Metrics capture a narrow aspect, often missing important qualitative aspects that are not fully automatable.
For instance, metrics such as pass@k do not reflect qualitative aspects of code such as maintainability, cognitive load, or readability.
These aspects are critical for the downstream task and influence the overall usability.
Moreover, benchmarks are isolated test sets and may not fully represent real-world applications.
For example, benchmarks like HumanEval synthesis code based on written specifications.
However, such explicit descriptions are rare in real-world applications.
Thus, evaluating the model performance with benchmarks only might not reflect real-world tasks and end-user usability.

% Overfitting to a specific task is a risk
Models may become overfitted to the training data, leading to poor performance on unseen data.
When optimizing for a metric, the underlying model might adjust its parameters to improve the metric, thereby failing to generalize.
This can result in a model performing exceptionally well on a benchmark but struggling to hold up the performance in unforeseen settings, such as real-world scenarios.

In many cases, the training data set for an LLM is not released in conjunction with the model.
The benchmark itself could be part of the model's training dataset.
Such benchmark leakage may lead to the model remembering the actual solution from the training data rather than solving the new task based on the seen data.
This leads to the model's artificially high performance on the benchmark; however, for unforeseen scenarios, the model performs poorly.
=======
=======
>>>>>>> 8e2d179 (Added first draft of benchmark+pass@k)

\subsubsection{Challenges}

\todo{write paragraph}


\subsubsection{Study Types}

\todo{Connect guideline to study types and for each type have bullet point lists with information that MUST, SHOULD, or MAY be reported (usage of those terms according to RFC 2119~\cite{rfc2119}).}

<<<<<<< HEAD
>>>>>>> fa2adc1 (Revise structure)
=======
=======
% Provide comparability
% Provide reproducability
% Track progress
% leaderboards
% Tool to select the best model for a given task

\subsubsection{Challenges}

% Bias throughs Python
The most prominent benchmarks for LLMs use Python, introducing a bias towards Python-based programs.
In code generation, HumanEval and MBPP are widely used benchmarks.
Since model performance is measured against these benchmarks, researchers often optimize for them.
As a result, performance may degrade if programming languages other than Python are used.

% Closed source vs open source
Many closed-source models, such as those released by OpenAI, achieve exceptional performance on certain tasks but lack transparency and reproducibility.
Benchmark leaderboards, particularly for code generation, are led by close-sourced models.
While researchers should compare performance against these models, they must consider that providers might discontinue them or apply undisclosed pre- or postprocessing beyond the researcher's control.
The lack of transparency introduces challenges in analyzing mistakes, limiting the understanding of model failures.
Open-source models offer greater transparency since the entire process is under the researcher's control; however, they require appropriate hardware.

% Not all metrics can be captured by fully automated metrics
% Benchmarks might not generalize to real-world tasks / Benchmarks only measure one aspect in an isolated setting
% Fail to capture other qualitative aspects (e.g. readability, maintainability)
Metrics capture a narrow aspect, often missing important qualitative aspects that are not fully automatable.
For instance, metrics such as pass@k do not reflect qualitative aspects of code such as maintainability, cognitive load, or readability.
These aspects are critical for the downstream task and influence the overall usability.
Moreover, benchmarks are isolated test sets and may not fully represent real-world applications.
For example, benchmarks like HumanEval synthesis code based on written specifications.
However, such explicit descriptions are rare in real-world applications.
Thus, evaluating the model performance with benchmarks only might not reflect real-world tasks and end-user usability.

% Overfitting to a specific task is a risk
Models may become overfitted to the training data, leading to poor performance on unseen data.
When optimizing for a metric, the underlying model might adjust its parameters to improve the metric, thereby failing to generalize.
This can result in a model performing exceptionally well on a benchmark but struggling to hold up the performance in unforeseen settings, such as real-world scenarios.

<<<<<<< HEAD
% Test data leakage
>>>>>>> c5e0665 (Added first draft of benchmark+pass@k)
<<<<<<< HEAD
>>>>>>> 8e2d179 (Added first draft of benchmark+pass@k)
=======
=======
In many cases, the training data set for an LLM is not released in conjunction with the model.
The benchmark itself could be part of the model's training dataset.
Such benchmark leakage may lead to the model remembering the actual solution from the training data rather than solving the new task based on the seen data.
This leads to the model's artificially high performance on the benchmark; however, for unforeseen scenarios, the model performs poorly.
>>>>>>> 2f1f7de (Added challenges)
>>>>>>> 9b86b4e (Added challenges)

\subsubsection{References}

\bibliographystyle{plain}
\bibliography{../../literature.bib}

\end{document}


<<<<<<< HEAD
<<<<<<< HEAD
=======
%% ## (WIP) Report Suitable Benchmarking Metrics

%% ### Metrics

%% LLM metrics capture an aspect of a model, capturing performance and quality of a model tailored for a specific task.
%% For example, one typical metric used for evaluating Large Language Model performance on code synthesis is pass@k.
%% For code synthesis, pass@k reports how the likelihood of a model to correctly complete a code snippet at least once within k tries.
%% However, pass@k is only one of many metrics uesd to evaluate code synthesis.

%% [HOUS at. el.](https://arxiv.org/pdf/2308.10620) reviewed 395 LLM for software engineering papers and reported the most used metrics for a problem type.
%% They categorized each metric in one of three problem categories: regression, classification and generation.
%% The table of used metrics for each category is shown below.

%% ![Usage of Evaluation Metrics](/assets/images/evaluation_metrics.png)
%% *Usage of Evaluation Metrics taken from [HOUS at. el.](https://arxiv.org/pdf/2308.10620)*

%% ***TODO: For each metric***
%% - [ ] Explain the metric in more detail?
%% - [ ] Explain what is considered "good" and "bad" for a given context?
%% - [ ] Add some examples
%% - [ ] Categorize metrics based on the paper in metrics used for regression, code generation and classification?
%% - [ ] Include mathematical notation? If yes, explain it in more detail to make it easier?/ provide python implementation/ example?

%% #### Code Generation

%% For code generation the following metrics are reported. ***TODO Note: I think we should only explain the top n***

>>>>>>> 8e2d179 (Added first draft of benchmark+pass@k)
=======
>>>>>>> 9b86b4e (Added challenges)
%% * [pass@k](https://arxiv.org/pdf/2107.03374) Pass@1 Pass@2 Pass@5 Pass@10, reporting of k depends on the specific usecase of the tool. e.g. for code completion in an IDE pass@1 is essential because the user only get to see one result, pass@5 and pass@10 show that LLMs are somewhat capable of solving the given task (indication that through e.g. fine-tuning it might be possible to solve) or if users get multiple options to choose from (e.g. test generation)
%% * [BLEU-N](https://aclanthology.org/P02-1040.pdf)
%% * [CodeBLEU](https://arxiv.org/abs/2009.10297)
%% * [CrystalBLEU](https://software-lab.org/publications/ase2022_CrystalBLEU.pdf)
%% * [ROUGE](https://aclanthology.org/W04-1013.pdf)
%% * Accuracy/Accuracy@k
%% * Mean Reciprocal Rank
%% * Edit Similarity (ES)
%% * Edit Distance (ED)
%% * Exact Match (EM)
%% * [METEOR](https://dl.acm.org/doi/pdf/10.5555/1626355.1626389)
%% * Recall
%% * F1-score
%% * Mean Reciprocal Rank (MRR)
%% * Mean Average Ranking (MAR)
%% * Mean First Ranking (MFR)
%% * [Character n-gram F-Score (ChrF)](https://aclanthology.org/W15-3049.pdf)
%% * [CodeBERTScore](https://arxiv.org/pdf/2302.05527)
%% * Perplexity (PP)


%% #### Domain-specific metrics
<<<<<<< HEAD
%%
=======
>>>>>>> 8e2d179 (Added first draft of benchmark+pass@k)
%% * If a tool is analyzed, the acceptance rate of generated artifacts could be interesting (how many artifacts were accepted/rejected by the user)
%% * Inter-model-agreement (related to section on open LLM as baseline): Ask different LLMs or differently confidered LLMs and determine their agreement
%% * Cross Cutting Metrics: Costs/Energy requirements to get the results (database community often reports on how expensive it was to get results, would be interesting for LLMs too but nobody does it yet)

%% #### Classification

%% - Precision
%% - Recall
%% - F1-Score
%% - Accuracy
%% - Area Under the Receiver Operating Characteristic (ROC) Curve
%% - Receiver Operating Characteristic (ROC)
%% - False Positive Rate (FPR)
%% - False Negative Rate (FNR)
%% - Matthews Correlation Coefficient (MCC)

%% #### Recommendation

%% - Mean Reciprocal Rank (MRR)
%% - Precision/Precision@k
%% - MAP/MAP@k
%% - F-score/F-score@k
%% - Recall/Recall@k
%% - Accuracy

%% ### Benchmarks

%% ***TODO:*** Maybe something along the lines of using different benchmarks? Being aware of their biases (e.g., focus on a particular programming language such as Python)?

%% * [HumanEval](https://github.com/openai/human-eval)
%% * [REPOCOD](https://huggingface.co/datasets/lt-asset/REPOCOD)
%% * [ClassEval](https://arxiv.org/abs/2308.01861)

%% #### Code Translation

%% * [Avatar](https://arxiv.org/pdf/2108.11590)
%% * [Transcoder](https://arxiv.org/pdf/2006.03511)
<<<<<<< HEAD

% #### Baselines

% Provide a reference point for existing research
% - often include traditional non-LLM based
% - used to show a difference between (often) low-cost solutions and
% - should report the same metric e.g.
% Example:
% - LLM for test prioritisation
% - LLM
=======
>>>>>>> 8e2d179 (Added first draft of benchmark+pass@k)

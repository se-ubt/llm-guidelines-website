\input{../../header.tex}

\begin{document}

\subsection{Use an Open LLM as a Baseline}

\subsubsection{Recommendations}

Empirical studies using LLMs in software engineering, especially the ones targeting commercial tools or model, \should incorporate an open LLM as a baseline and report established metrics for inter-model agreement (see Section \benchmarksmetrics).
\todo{Sebastian: Please check that the cited section contains the required information. If not, add it there.}
We acknowledge that including an open LLM baseline might not always be possible, for example if the study involves human participants and letting them work on the tasks using two different models might not be feasible.
However, open models allow other researchers to verify research results and build upon them, even without access to commercial models.
Comparing commercial and open models also allows to contextualize model performance.
Researchers \should provide a full replication package as part of their \supplementarymaterial, including clear, step-by-step instructions on how to verify and reproduce the results reported in the paper.

Open LLMs are available on platforms such as \href{https://huggingface.co/}{Hugging Face}.
Depending on the size, and the local computing power, open LLMs can be hosted on a local computer or server using frameworks such as \href{https://ollama.com/}{Ollama} or \href{https://lmstudio.ai/}{LM Studio}.
They can also be run on cloud-based services such as \href{https://together.ai/}{Together AI} or large hyperscalers such as AWS, Azure, Alibaba Cloud, and Google Cloud.

The term ``open'' can have different meanings in context of LLMs.
Widder et al. discuss three types of openness: transparency, reusability and extensibility~\cite{widder2024open}.
They also discuss what openness in AI can and cannot provide.
Moreover, the Open Source Initiative (OSI) \cite{OSIAI2024} provides a definition of open-source AI that serves as a useful framework for evaluating the openness of AI models.
In simple terms, according to OSI, open-source AI means  you have access to everything you need to use the AI, such as understanding, modifying, sharing, retraining, and recreating.


\subsubsection{Example(s)}

\todo{Sebastian: Artificial examples should only be used in case we can't find any SE or non-SE papers that use open LLMs as a baseline. Please add suitable examples here.}

\begin{itemize}
    \item Benchmarking a Proprietary LLM: Researchers want to know how good their own LLM is at writing code. They might compare it against an open LLM such as StarCoderBase.
    \item Evaluating an LLM-Powered Tool: A team developing an AI-driven code review tool might want to assess the quality of suggestions generated by both a proprietary LLM and an open alternative. Human evaluators could then independently rate the relevance and correctness of the suggestions, providing an objective measure of the tool's effectiveness.
    \item Ensuring Reproducibility with a Replication Package: A study on bug localization that uses a closed-source LLM could support Reproducibility by including a replication package. This package might contain a script that automatically reruns the same experiments using an open-source LLM—such as Llama 3—and generates a comparative report.
\end{itemize}

\subsubsection{Advantages}

\todo{Sebastian: Please formulate a text based on the bullet points (see earlier guidelines for inspiration).}

\begin{itemize}
    \item	Improved Reproducibility: Researchers can independently replicate experiments.
    \item	More Objective Comparisons: Using a standardized baseline allows for more unbiased evaluations.
    \item	Greater Transparency: Open models enable the analysis of how data is processed, which supports researchers in identifying potential biases and limitations.
    \item	Long-Term Accessibility: Unlike proprietary models, which may become unavailable, open LLMs remain available for future studies.
    \item	Lower Costs: Open-source models usually have fewer licensing restrictions, which makes them more accessible to researchers with limited funding.
\end{itemize}


\subsubsection{Challenges}

\todo{Sebastian: Please formulate a text based on the bullet points (see earlier guidelines for inspiration).}

\begin{itemize}
    \item Performance Differences: Open models may not always match the latest proprietary LLMs in accuracy or efficiency, making it harder to demonstrate improvements.
    \item Computational Demands: Running large open models requires hardware resources, including high-performance GPUs and significant memory.
    \item Defining "Openness": The term open is evolving—many so-called open models provide access to weights but do not disclose training data or methodologies. We are aware that the definition of an ``open'' model is actively being discussed, and many open models are essentially only ``open weight''~\cite{Gibney2024}.
    \item We consider the \emph{Open Source AI Definition} proposed by the \emph{Open Source Initiative} (OSI)~\cite{OSIAI2024} to be a first step towards defining true open-source models.
    \item Implementation Complexity: Unlike cloud-based APIs from proprietary providers, setting up and fine-tuning open models can be technically demanding due to the possible limited documentation.
\end{itemize}

\subsubsection{Study Types}

\todo{Sebastian: Please formulate a text based on the bullet points (see earlier guidelines for inspiration) and use the command from \texttt{header.tex} to refer to the study types.}

\begin{itemize}
    \item Tool Evaluation: An open LLM baseline \must be included if technically feasible. If integration is too complex, researchers \should at least report initial benchmarking results using open models.
    \item Benchmarking Studies and Controlled Experiments: An open LLM \must be one of the models evaluated.
    \item Observational Studies: If an open LLM is impossible, the researchers \should acknowledge its absence and discuss potential impacts on their findings.
    \item Qualitative Studies: If the LLM is used for exploratory data analysis or to compare alternative interpretations of results then an LLM baseline \may be reported.
\end{itemize}

\subsubsection{References}

\bibliographystyle{plainnat}
\bibliography{../../literature.bib}

\end{document}
